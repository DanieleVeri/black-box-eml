{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import numpy as np\n",
    "from emlopt import solvers, surrogates\n",
    "from emlopt.utils import *\n",
    "from emlopt.problem import build_problem\n",
    "from experiments.problems.simple_functions import polynomial, build_rosenbrock, mccormick\n",
    "\n",
    "import logging\n",
    "def create_logger(name):\n",
    "    test_logger = logging.getLogger(name)\n",
    "    stream = logging.StreamHandler(sys.stdout)\n",
    "    stream.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "    if not test_logger.handlers:\n",
    "        test_logger.addHandler(stream)\n",
    "    test_logger.setLevel(logging.DEBUG)\n",
    "    test_logger.propagate = False\n",
    "    return test_logger\n",
    "logger = create_logger('ebm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_probability as tfp\n",
    "from emlopt.surrogates.base_surrogate import BaseSurrogate\n",
    "from emlopt.utils import min_max_scale_in, min_max_scale_out\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import math\n",
    "\n",
    "def build_regressor(input_shape, depth=4, width=20):\n",
    "    mdl = tf.keras.Sequential()\n",
    "    mdl.add(tf.keras.layers.Input(shape=(input_shape,), dtype='float32'))\n",
    "    for i in range(depth):\n",
    "        mdl.add(tf.keras.layers.Dense(width, activation='relu'))\n",
    "    mdl.add(tf.keras.layers.Dense(2, activation='linear'))\n",
    "    return mdl\n",
    "\n",
    "def shuffle_dataset(dataset_x, dataset_y):\n",
    "    dataset_y = np.expand_dims(dataset_y, axis=-1)\n",
    "    concatenated = np.concatenate((dataset_x,dataset_y), axis=-1)\n",
    "    np.random.shuffle(concatenated)\n",
    "    return concatenated[:,:dataset_x.shape[1]], concatenated[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def logprob(x, mean, std):\n",
    "    log_unnormalized = -0.5 * tf.math.squared_difference(x / std, mean / std)\n",
    "    log_normalization = tf.constant(0.5 * np.log(2. * np.pi), dtype=np.float) + tf.math.log(std)\n",
    "    return log_unnormalized - log_normalization\n",
    "    \n",
    "\n",
    "def normal_logprob_loss(batchX, batchY, keras_mdl):\n",
    "    p = keras_mdl(batchX)\n",
    "    pmean = p[:,0]\n",
    "    plogstd = p[:,1]\n",
    "    pstd = tf.math.exp(plogstd)\n",
    "    y_logprob = logprob(batchY, pmean, pstd)\n",
    "    return tf.math.reduce_mean(-y_logprob)\n",
    "\n",
    "\n",
    "def mse_loss(batchX, batchY, keras_mdl):\n",
    "    p = keras_mdl(batchX)\n",
    "    pmean = p[:,0]\n",
    "    plogstd = p[:,1]\n",
    "    pstd = tf.math.exp(plogstd)\n",
    "    l = tf.math.squared_difference(pmean, batchY)\n",
    "    return tf.math.reduce_mean(l)\n",
    "\n",
    "\n",
    "def gauss_density_centered(x, std):\n",
    "    return tf.math.exp(-0.5*(x / std)**2) / (math.sqrt(2*math.pi)*std)\n",
    "\n",
    "def partition_loss(batchX, batchY, keras_mdl):\n",
    "    stds = [0.01, 0.1]\n",
    "    loss = 0\n",
    "    for std in stds:\n",
    "        noise = tf.random.normal(batchX.shape, 0, std)\n",
    "        q = batchX + noise\n",
    "        q_hat = keras_mdl(q)\n",
    "        q_hat_std = tf.clip_by_value(q_hat[:,1], clip_value_min=-math.log(10), clip_value_max=math.log(10))\n",
    "        q_hat_std = tf.math.exp(q_hat_std)\n",
    "        w = gauss_density_centered(noise, std)\n",
    "        loss += -tf.math.reduce_mean(tf.math.log(q_hat_std/w))\n",
    "    return loss / len(stds)\n",
    "\n",
    "\n",
    "def custom_loss(batchX, batchY, keras_mdl):\n",
    "    return (\n",
    "        normal_logprob_loss(batchX, batchY, keras_mdl) + \n",
    "        mse_loss(batchX, batchY, keras_mdl) +  \n",
    "        partition_loss(batchX, batchY, keras_mdl) \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop and plot util\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class EBM(BaseSurrogate):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(EBM, self).__init__(*args, **kwargs)\n",
    "        \n",
    "    def fit_surrogate(self, x, y):\n",
    "        bs = self.batch_size if self.batch_size else x.shape[0]\n",
    "\n",
    "        y_lb = self.problem.y_lb\n",
    "        y_ub = self.problem.y_ub\n",
    "\n",
    "        norm_x = min_max_scale_in(x, np.array(self.problem.input_bounds))\n",
    "        norm_y = (y - y_lb) / (y_ub - y_lb)\n",
    "\n",
    "        # optimizer = tfa.optimizers.AdamW(\n",
    "        #     weight_decay=self.weight_decay, learning_rate=self.lr)\n",
    "\n",
    "        optimizer = tf.optimizers.Adam(learning_rate=self.lr)\n",
    "\n",
    "        keras_mdl = build_regressor(self.problem.input_shape, self.depth, self.width)\n",
    "                                                                                                       \n",
    "        history = []\n",
    "        for epoch in range(self.epochs):\n",
    "            print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "            bs = min(norm_x.shape[0], self.batch_size)\n",
    "            norm_x, norm_y = shuffle_dataset(norm_x, norm_y)\n",
    "\n",
    "            for batch_idx in range(math.ceil(norm_x.shape[0]/bs)):\n",
    "                if batch_idx == math.ceil(norm_x.shape[0]/bs)-1:\n",
    "                    batchX = norm_x[batch_idx*bs:]\n",
    "                    batchY = norm_y[batch_idx*bs:]\n",
    "                else:\n",
    "                    batchX = norm_x[batch_idx*bs:(batch_idx+1)*bs]\n",
    "                    batchY = norm_y[batch_idx*bs:(batch_idx+1)*bs]\n",
    "                with tf.GradientTape() as tape:\n",
    "                    loss_value = custom_loss(batchX, batchY, keras_mdl)\n",
    "                    grads = tape.gradient(loss_value, keras_mdl.trainable_weights)\n",
    "                optimizer.apply_gradients(zip(grads, keras_mdl.trainable_weights))\n",
    "                \n",
    "            print(\"Training loss \", loss_value)\n",
    "            history.append(loss_value)\n",
    "            \n",
    "        plt.plot(history)\n",
    "        plt.show()\n",
    "        \n",
    "        return keras_mdl\n",
    "\n",
    "    def plot_predictions(self, keras_mdl, samples_x, samples_y):\n",
    "        y_ub = self.problem.y_ub\n",
    "        y_lb = self.problem.y_lb\n",
    "        if self.problem.input_shape <= 2:\n",
    "            x, y = self.problem.get_grid(100)\n",
    "            scaled_x = min_max_scale_in(x, np.array(self.problem.input_bounds))\n",
    "\n",
    "            prob_pred = keras_mdl(scaled_x)\n",
    "\n",
    "            pred = prob_pred[:,0].numpy().ravel()\n",
    "            pred = pred * (y_ub - y_lb) + y_lb\n",
    "            std_pred = tf.math.exp(prob_pred[:,1]).numpy().ravel()\n",
    "            std_pred = std_pred * (y_ub - y_lb) \n",
    "\n",
    "        # 1D domain\n",
    "        if self.problem.input_shape == 1:\n",
    "            fig = plt.figure(figsize=(15, 10))\n",
    "            plt.xlim(self.problem.input_bounds[0])\n",
    "            x = np.squeeze(x)\n",
    "            plt.plot(x, y, c=\"grey\")\n",
    "            plt.plot(x, pred)\n",
    "            plt.fill_between(x, pred-std_pred, pred+std_pred, alpha=0.3, color='tab:blue', label='+/- std')\n",
    "            plt.scatter(samples_x, samples_y, c=\"orange\")\n",
    "            plt.legend([\"GT\", \"predicted mean\", \"predicted CI\", \"samples\"], prop={'size': 14})\n",
    "            plt.savefig(f'{tempfile.gettempdir()}/chart.png')\n",
    "            if is_plot_visible(): plt.show()\n",
    "            else: plt.close()\n",
    "\n",
    "        # 2D domain\n",
    "        elif self.problem.input_shape == 2:\n",
    "            fig = plt.figure(figsize=(15, 10))\n",
    "            ax = fig.add_subplot(111, projection='3d')\n",
    "            ax.scatter(\n",
    "                samples_x[:, 0], samples_x[:, 1], samples_y, color=\"orange\")\n",
    "            ax.scatter(x[:, 0], x[:, 1], y, alpha=0.15, color=\"lightgrey\")\n",
    "            ax.scatter(x[:, 0], x[:, 1], pred, alpha=0.3)\n",
    "            ax.scatter(x[:, 0], x[:, 1], pred-std_pred, alpha=0.3, color=\"lightblue\")\n",
    "            ax.scatter(x[:, 0], x[:, 1], pred+std_pred, alpha=0.3, color=\"lightblue\")\n",
    "            ax.view_init(elev=15, azim=60)\n",
    "            plt.legend([\"samples\", \"GT\", \"predicted mean\", \"predicted CI\"], prop={'size': 14})\n",
    "            plt.savefig(f'{tempfile.gettempdir()}/chart.png')\n",
    "            if is_plot_visible(): plt.show()\n",
    "            else: plt.close()\n",
    "\n",
    "        else:\n",
    "            self.logger.debug(\"Plot not available for high dimensional domains.\")\n",
    "            self.logger.debug(f\"X:\\n{samples_x}\\nY:\\n{samples_y}\")\n",
    "\n",
    "            scaled_samples_x = min_max_scale_in(samples_x,np.array(self.problem.input_bounds))\n",
    "            prob_pred = keras_mdl(scaled_samples_x)\n",
    "            mu = min_max_restore_out(prob_pred[:,0].numpy().ravel(), samples_y)\n",
    "\n",
    "            self.logger.debug(f\"mu pred\\n{mu}\")\n",
    "            self.logger.debug(f\"diff\\n{samples_y-mu}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset: Hole in the middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "def linear_constraint(backend, model, xvars):\n",
    "    x=xvars[0]\n",
    "    return [\n",
    "        [x<= 0.7 , \"center_dist\"],\n",
    "        [x>= 0.3 , \"center_dist2\"]\n",
    "    ]\n",
    "\n",
    "problem = build_problem(\"polynomial_1D\", lambda x: math.sin(x*10)/x/10, ['real'], [[-1,1]])\n",
    "dataset = problem.get_dataset(200, backend_type='cplex')\n",
    "problem.y_lb = -1\n",
    "problem.y_ub = 1\n",
    "\n",
    "X, Y = [], []\n",
    "for ii, aa in enumerate(dataset[0]):\n",
    "    if (aa > -0.5 and aa < -0.3) or aa > 0.75:\n",
    "        X.append(aa)\n",
    "        Y.append(dataset[1][ii])\n",
    "X = np.stack(X)\n",
    "Y = np.stack(Y)\n",
    "\n",
    "surrogate_cfg = {\n",
    "    \"epochs\": 200,\n",
    "    \"learning_rate\": 5e-3,\n",
    "    \"weight_decay\": 0,\n",
    "    \"batch_size\": 32,\n",
    "    \"depth\": 3,\n",
    "    \"width\": 40,\n",
    "}\n",
    "\n",
    "surrogate_model = EBM(problem, surrogate_cfg, logger)\n",
    "mdl = surrogate_model.fit_surrogate(X,Y)\n",
    "\n",
    "surrogate_model.plot_predictions(mdl, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset: Sinc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "problem = build_problem(\"polynomial_1D\", lambda x: -math.sin(x*10)/x/20, ['real'], [[-1,1]])\n",
    "dataset = problem.get_dataset(200, backend_type='cplex')\n",
    "problem.y_lb = -1\n",
    "problem.y_ub = 1\n",
    "\n",
    "surrogate_cfg = {\n",
    "    \"epochs\": 200,\n",
    "    \"learning_rate\": 5e-3,\n",
    "    \"weight_decay\": 0,\n",
    "    \"batch_size\": 32,\n",
    "    \"depth\": 3,\n",
    "    \"width\": 40,\n",
    "}\n",
    "\n",
    "surrogate_model = EBM(problem, surrogate_cfg, logger)\n",
    "mdl = surrogate_model.fit_surrogate(*dataset)\n",
    "\n",
    "surrogate_model.plot_predictions(mdl, *dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset: Ackley 10D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "from experiments.problems.simple_functions import build_ackley\n",
    "\n",
    "def constraint_scbo(backend, milp_model, xvars):\n",
    "    r = 5\n",
    "    acc1 = 0\n",
    "    for xi in xvars:\n",
    "        acc1 += xi*xi\n",
    "    acc2 = 0\n",
    "    for xi in xvars:\n",
    "        acc2 += xi\n",
    "    return [[acc1 <= r*r, \"center_dist\"], [acc2 <= 0, \"cst2\"]]\n",
    "\n",
    "problem = build_problem(\"ackley_10d_cst\", *build_ackley(10), constraint_scbo)\n",
    "dataset = problem.get_dataset(30, backend_type='cplex')\n",
    "problem.y_lb = 0\n",
    "problem.y_ub = 1000\n",
    "\n",
    "surrogate_cfg = {\n",
    "    \"epochs\": 1000,\n",
    "    \"learning_rate\": 5e-3,\n",
    "    \"weight_decay\": 0,\n",
    "    \"batch_size\": 64,\n",
    "    \"depth\": 2,\n",
    "    \"width\": 40,\n",
    "}\n",
    "\n",
    "surrogate_model = EBM(problem, surrogate_cfg, logger)\n",
    "mdl = surrogate_model.fit_surrogate(*dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EML embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from emlopt.solvers.base_milp import BaseMILP\n",
    "from emlopt.emllib.backend import Backend, get_backend\n",
    "from emlopt.emllib.net.reader.keras_reader import read_keras_sequential\n",
    "from emlopt.emllib.net.process import fwd_bound_tighthening, ibr_bounds\n",
    "from emlopt.emllib.net import embed\n",
    "from emlopt.emllib.util import pwl_exp\n",
    "\n",
    "\n",
    "class UCB_EBM(BaseMILP):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(UCB_EBM, self).__init__(*args, **kwargs)\n",
    "        self.lambda_ucb = self.cfg['lambda_ucb']\n",
    "\n",
    "    @timer\n",
    "    def propagate_bound(self, parsed_model, timeout=30):\n",
    "        backend = get_backend(self.cfg['backend'])\n",
    "        bounds = np.array([[0,1]]*(self.problem.input_shape+1)) # input shape + y\n",
    "        parsed_model.layer(0).update_lb(bounds[:,0])\n",
    "        parsed_model.layer(0).update_ub(bounds[:,1])\n",
    "        method = self.cfg.get('bound_propagation', 'both') # both is default\n",
    "        if method == 'ibr':\n",
    "            self.logger.debug(f\"Using Interval Based Reasoning bound propagation\")\n",
    "            ibr_bounds(parsed_model)\n",
    "        elif method == 'milp':\n",
    "            self.logger.debug(f\"Using MILP forward bound tighthening bound propagation\")\n",
    "            fwd_bound_tighthening(backend, parsed_model, timelimit=timeout)\n",
    "        elif method == 'both':\n",
    "            self.logger.debug(f\"Using IBR and then MILP bound propagation\")\n",
    "            ibr_bounds(parsed_model)\n",
    "            fwd_bound_tighthening(backend, parsed_model, timelimit=timeout)\n",
    "        else:\n",
    "            raise Exception('Invalid bound propagation method')\n",
    "        return parsed_model\n",
    "\n",
    "    def embed_model(self, bkd, milp_model, parsed_model, new_bounds=None, name=\"\"):\n",
    "        # bounds computed with propagate bounds method\n",
    "        score_lb = parsed_model.layer(-1).lb()[0].item()\n",
    "        score_ub = parsed_model.layer(-1).ub()[0].item()\n",
    "        self.logger.debug(f\"Computed bounds: lb: {score_lb} ub: {score_ub}\")\n",
    "        # x decision variables\n",
    "        xvars = []\n",
    "        norm_xvars = []\n",
    "        bounds = self.problem.input_bounds if new_bounds is None else new_bounds\n",
    "        for i,b in enumerate(bounds):\n",
    "            if self.problem.input_type[i] == \"int\":\n",
    "                xvars.append(bkd.var_int(milp_model, lb=b[0], ub=b[1], name=f\"{name}_x{str(i)}\"))\n",
    "            else:\n",
    "                xvars.append(bkd.var_cont(milp_model, lb=b[0], ub=b[1], name=f\"{name}_x{str(i)}\"))\n",
    "            # NN scaled input\n",
    "            norm_xvars.append(bkd.var_cont(milp_model, lb=0, ub=1, name=f\"{name}_norm_x{str(i)}\"))\n",
    "            bkd.cst_eq(milp_model, norm_xvars[-1] * (b[1] - b[0]), xvars[-1] - b[0], f\"{name}_cst_norm_x{str(i)}\")\n",
    "        # y variable\n",
    "        yvar = bkd.var_cont(milp_model, lb=self.problem.y_lb, ub=self.problem.y_ub, name=f\"{name}_y\")\n",
    "        norm_yvar = bkd.var_cont(milp_model, lb=0, ub=1, name=f\"{name}_norm_y\")\n",
    "        bkd.cst_eq(milp_model, norm_yvar * (self.problem.y_ub - self.problem.y_lb), yvar - self.problem.y_lb, f\"{name}_cst_norm_y\")\n",
    "        # score variable\n",
    "        scorevar = bkd.var_cont(milp_model, lb=score_lb, ub=score_ub, name=f\"{name}_score\")\n",
    "        norm_scorevar = bkd.var_cont(milp_model, lb=0, ub=1, name=f\"{name}_norm_score\")\n",
    "        bkd.cst_eq(milp_model, norm_scorevar * (score_ub - score_lb), scorevar - score_lb, f\"{name}_cst_norm_score\")\n",
    "\n",
    "        embed.encode(bkd, parsed_model, milp_model, norm_xvars + [norm_yvar], [scorevar], name)\n",
    "        return xvars, norm_xvars, yvar, norm_yvar, scorevar, norm_scorevar\n",
    "\n",
    "    def extract_solution(self, solution_vars, name, scaled=False):\n",
    "        opt_x = np.zeros(self.problem.input_shape)\n",
    "        for i in range(self.problem.input_shape):\n",
    "            if scaled:\n",
    "                opt_x[i] = solution_vars[f\"{name}_norm_x\"+str(i)]\n",
    "            else:\n",
    "                opt_x[i] = solution_vars[f\"{name}_x\"+str(i)]\n",
    "        return opt_x\n",
    "\n",
    "    def solve(self, keras_model, samples_x, samples_y):\n",
    "        bkd = get_backend(self.cfg['backend'])\n",
    "        milp_model = bkd.new_model()\n",
    "\n",
    "        parsed_mdl = read_keras_sequential(keras_model)\n",
    "        parsed_mdl, _ = self.propagate_bound(parsed_mdl, timeout=30, timer_logger=self.logger)\n",
    "\n",
    "        # embed 2 times the nn\n",
    "        xvars, norm_xvars, yvar, norm_yvar, scorevar, norm_scorevar = self.embed_model(bkd, milp_model, parsed_mdl, name='nn1')\n",
    "        # xvars_2, norm_xvars_2, yvar_2, norm_yvar_2, scorevar_2, norm_scorevar_2 = self.embed_model(bkd, milp_model, parsed_mdl, name='nn2')\n",
    "        \n",
    "        # bound the X to be the same\n",
    "        # for ii,xx in enumerate(xvars):\n",
    "        #     bkd.cst_eq(milp_model, xvars[ii], xvars_2[ii], \"same_X\")\n",
    "\n",
    "        # bound the y to be different\n",
    "        # y_delta = bkd.var_cont(milp_model, lb=0, ub=1, name=\"y_explorer\")\n",
    "        # bkd.add_cst(milp_model, norm_yvar >= y_delta+norm_yvar_2, \"y_delta\")\n",
    "\n",
    "        # bound the scores to be different\n",
    "        # s_delta = bkd.var_cont(milp_model, lb=0, ub=1, name=\"s_explorer\")\n",
    "        # bkd.add_cst(milp_model, norm_scorevar >= s_delta+norm_scorevar_2, \"s_delta\")\n",
    "\n",
    "        # PI\n",
    "        # current_min = np.min(samples_y)\n",
    "        # bkd.add_cst(milp_model, yvar <= current_min, \"pi\")\n",
    "        # bkd.add_cst(milp_model, norm_scorevar >= 0.5, \"pi\")\n",
    "\n",
    "        # objective\n",
    "        objective = norm_scorevar - norm_yvar\n",
    "\n",
    "        if self.problem.constraint_cb is not None:\n",
    "            csts = self.problem.constraint_cb(bkd, milp_model, xvars)  #TODO: cst xvar2 too\n",
    "            for pc in csts:\n",
    "                bkd.add_cst(milp_model, *pc)\n",
    "\n",
    "        bkd.set_obj(milp_model, 'max', objective)\n",
    "        bkd.set_determinism(milp_model)\n",
    "        if self.logger.level == logging.DEBUG:\n",
    "            bkd.set_extensive_log(milp_model)\n",
    "        solution = bkd.solve(milp_model, self.solver_timeout)\n",
    "\n",
    "        if solution['status'] == 'infeasible':\n",
    "            raise Exception(\"Not feasible\")\n",
    "\n",
    "        # self.logger.debug(f\"Solution: {solution}\")\n",
    "        # decision_variables = self.extract_solution(solution['vars'], \"nn1\")\n",
    "        main_variables = {\n",
    "            \"objective\": solution['obj'],\n",
    "            \"x\": solution['vars']['nn1_x0'],\n",
    "            \"norm_score1\": solution['vars']['nn1_norm_score'],\n",
    "            # \"norm_score2\": solution['vars']['nn2_norm_score'],\n",
    "            \"y1\": solution['vars']['nn1_y'],\n",
    "            # \"y2\": solution['vars']['nn2_y'],\n",
    "            # \"norm y delta\": solution['vars']['y_explorer'],\n",
    "            # \"s delta\": solution['vars']['s_explorer']\n",
    "        }\n",
    "        if self.solution_callback is not None:\n",
    "            self.solution_callback(main_variables, solution)\n",
    "\n",
    "        #return decision_variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "cfg = {\"backend\": 'cplex', \"lambda_ucb\": 1, \"solver_timeout\": 30}\n",
    "\n",
    "def cb(main_variables, all_variables):\n",
    "    logger.debug(main_variables)\n",
    "\n",
    "milp_model = UCB_EBM(problem, cfg, 1, logger)\n",
    "milp_model.solution_callback = cb\n",
    "milp_model.optimize_acquisition_function(mdl, *dataset, timer_logger=logger)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "nNNmQ5x5YJaS",
    "JcyNrxy43Bgb"
   ],
   "name": "release.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "vscode": {
   "interpreter": {
    "hash": "87c8eeea08ab3d29ad729688c7dc8d710eb7504e2022354839d375ec51a3ad09"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
