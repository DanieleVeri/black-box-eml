{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hz2foCh7eqQr"
      },
      "source": [
        "# Empirical Model Learning for Black Box Optimization\n",
        "Master Thesis in Artificial Intelligence at University of Bologna, a.y. 2021/2022\n",
        "\n",
        "Daniele Verì, Michele Lombardi, Andrea Borghesi, Stefano Teso\n",
        "___\n",
        "This notebook is diveded in 4 sections:\n",
        "- 📥 Dependencies: downloads and install the requirements\n",
        "- 📑 Definitions: define classes and functions used\n",
        "- 🚀 Experiments: set parameters, instance targets and run the experiments\n",
        "- 🧪 Test: test the proper functionality of class and functions\n",
        "\n",
        "Name convenctions adopted are:\n",
        "- Functions and local variable are in snake case\n",
        "- Class are in camel case\n",
        "- Global variables are in caps lock\n",
        "\n",
        "📈 You can keep track of the experimets using Weights and Biases.  \n",
        "**If you don't have an account, disable the corresponding checkbox.**\n",
        "\n",
        "In order to achieve the true determinism, you have to perform the training on CPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNNmQ5x5YJaS"
      },
      "source": [
        "# 📥 Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCrU3TGEYlBp"
      },
      "outputs": [],
      "source": [
        "#@title GPyopt\n",
        "!pip install GPyOpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsM5SqajYG_V"
      },
      "outputs": [],
      "source": [
        "#@title CPLEX\n",
        "\n",
        "!wget https://api.wandb.ai/artifactsV2/gcp-us/veri/QXJ0aWZhY3Q6MjU1ODAzMjI=/69b1b89a73a7d0931fbfdb355eb147c3 -O cplex_studio1210.linux-x86-64.bin\n",
        "!wget https://api.wandb.ai/artifactsV2/gcp-us/veri/QXJ0aWZhY3Q6MjU1ODAzMjI=/97133b747b0114a4e3dba77ab26d68d5 -O response.properties\n",
        "\n",
        "!pip install docplex\n",
        "!sh cplex_studio1210.linux-x86-64.bin -f response.properties\n",
        "!python3 /opt/ibm/ILOG/CPLEX_Studio1210/python/setup.py install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkPqw-nj0X3v"
      },
      "outputs": [],
      "source": [
        "#@title Tensorflow addons\n",
        "\n",
        "!pip install tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BaPHqIopVAWW"
      },
      "outputs": [],
      "source": [
        "#@title EMLlib\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "!git clone https://github.com/DanieleVeri/emllib.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nsfFSUmkCr4"
      },
      "outputs": [],
      "source": [
        "#@title scikit-optimize\n",
        "!pip install scikit-optimize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title tensorflow quantization\n",
        "!pip install -q tensorflow-model-optimization"
      ],
      "metadata": {
        "id": "KWM9RftUkuel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcyNrxy43Bgb"
      },
      "source": [
        "# 📑 Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2SDDKf13x94"
      },
      "outputs": [],
      "source": [
        "#@title Base import and seed\n",
        "\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import sys\n",
        "if not 'emllib' in sys.path: sys.path.insert(1, 'emllib')\n",
        "\n",
        "import pickle\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    tf.compat.v1.set_random_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "admZL6drfYtp"
      },
      "source": [
        "##Problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "_X0Nzl3PeFAw"
      },
      "outputs": [],
      "source": [
        "#@title Problem class - quadratic + random slack\n",
        "\n",
        "import docplex.mp.model as cpx\n",
        "\n",
        "from skopt.sampler import Lhs\n",
        "from skopt.space import Space\n",
        "\n",
        "class ProblemQRS:\n",
        "\n",
        "    def __init__(self, name, fun, input_type, input_bounds, constraint_cb=None, stocasthic=False):\n",
        "        self.name = name\n",
        "        self.fun = fun\n",
        "        self.input_type = input_type\n",
        "        self.input_bounds = input_bounds\n",
        "        self.input_shape = len(self.input_bounds)\n",
        "        self.constraint_cb = constraint_cb\n",
        "        self.stocasthic = stocasthic\n",
        "\n",
        "    def get_dataset(self, n_points):\n",
        "        if self.constraint_cb is None:\n",
        "            x = np.random.rand(n_points, self.input_shape)\n",
        "            for i, b in enumerate(self.input_bounds):\n",
        "                lb = b[0]\n",
        "                ub = b[1]\n",
        "                if self.input_type[i] == \"int\":\n",
        "                    x[:,i] = np.random.randint(lb, high=ub, size=n_points)\n",
        "                else:\n",
        "                    x[:,i] *= ub - lb\n",
        "                    x[:,i] += lb\n",
        "            y = np.zeros((n_points))\n",
        "            for i in range(n_points):\n",
        "                y[i] = self.fun(x[i, :])\n",
        "            return x, y\n",
        "\n",
        "        # constrained dataset:\n",
        "        x = np.zeros((n_points, self.input_shape))\n",
        "        bounds = []\n",
        "        for i, b in enumerate(self.input_bounds):\n",
        "            if self.input_type[i] == \"int\":\n",
        "                bounds.append((int(b[0]), int(b[1])))\n",
        "            else:\n",
        "                bounds.append((float(b[0]), float(b[1])))\n",
        "                \n",
        "        space = Space(bounds)\n",
        "        lhs = Lhs(lhs_type=\"classic\", criterion=None, iterations=1000)\n",
        "        lhs_samples = lhs.generate(space.dimensions, n_points)\n",
        "\n",
        "        p = 0\n",
        "        while True:\n",
        "            cplex = cpx.Model()\n",
        "            xvars = []\n",
        "            for i,b in enumerate(self.input_bounds):\n",
        "                if self.input_type[i] == \"int\":\n",
        "                    xvars.append(cplex.integer_var(lb=b[0], ub=b[1], name=\"x\"+str(i)))\n",
        "                else:\n",
        "                    xvars.append(cplex.continuous_var(lb=b[0], ub=b[1], name=\"x\"+str(i)))\n",
        "            \n",
        "            csts = self.constraint_cb(cplex, xvars)\n",
        "            # add slack\n",
        "            avg_range = np.mean(np.diff(np.array(self.input_bounds), axis=1))\n",
        "            for pc in csts:\n",
        "                if pc[0].sense.value == 1: # <=\n",
        "                    if pc[0].right_expr.is_constant() and not pc[0].right_expr.is_zero():\n",
        "                        pc[0].right_expr -= np.random.uniform()*pc[0].right_expr\n",
        "                    else:\n",
        "                        pc[0].right_expr -= np.random.uniform()*avg_range\n",
        "                elif pc[0].sense.value == 3: # >=\n",
        "                    if pc[0].right_expr.is_constant() and not pc[0].right_expr.is_zero():\n",
        "                        pc[0].right_expr += np.random.uniform()*pc[0].right_expr\n",
        "                    else:\n",
        "                        pc[0].right_expr += np.random.uniform()*avg_range\n",
        "                print(\"\\n\", pc[0])\n",
        "                cplex.add_constraint(*pc)\n",
        "\n",
        "            ## quadratic random objective (boundaries)\n",
        "            obj = 0\n",
        "            for i, var in enumerate(xvars):\n",
        "                obj += (var-lhs_samples[p][i]) ** 2\n",
        "            cplex.set_objective(\"min\", obj)\n",
        "            \n",
        "            # solve\n",
        "            cplex.set_time_limit(30)\n",
        "            sol = cplex.solve()\n",
        "            if sol is None:\n",
        "                print(\"infeasible\")\n",
        "                continue\n",
        "            for i in range(self.input_shape):\n",
        "                x[p, i] = sol[\"x\"+str(i)]\n",
        "\n",
        "            p += 1\n",
        "            if p == n_points: break\n",
        "        print(\"Initial points:\\n\", x)\n",
        "\n",
        "        # eval fun\n",
        "        y = np.zeros((n_points))\n",
        "        for i in range(n_points):\n",
        "            y[i] = self.fun(x[i, :])\n",
        "\n",
        "        return x,y\n",
        "\n",
        "    def get_grid(self, n_points):\n",
        "        x_list = []\n",
        "        for i, b in enumerate(self.input_bounds):\n",
        "            lb = b[0]\n",
        "            ub = b[1]\n",
        "            if self.input_type[i] == \"int\":\n",
        "                x_list.append(np.arange(lb, ub, max(1, (ub-lb)//n_points)))\n",
        "            else:\n",
        "                x_list.append(np.arange(lb, ub, (ub-lb)/n_points))\n",
        "        x = np.array(np.meshgrid(*x_list)).reshape(self.input_shape,-1).T\n",
        "        y = np.zeros((x.shape[0]))\n",
        "        for i in range(x.shape[0]):\n",
        "            y[i] = self.fun(x[i, :])\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "95KSPh8oFams"
      },
      "outputs": [],
      "source": [
        "#@title Problem class - convex interpolation\n",
        "\n",
        "import docplex.mp.model as cpx\n",
        "import pandas as pd\n",
        "\n",
        "class Problem:\n",
        "\n",
        "    def __init__(self, name, fun, input_type, input_bounds, constraint_cb=None, stocasthic=False):\n",
        "        self.name = name\n",
        "        self.fun = fun\n",
        "        self.input_type = input_type\n",
        "        self.input_bounds = input_bounds\n",
        "        self.input_shape = len(self.input_bounds)\n",
        "        self.constraint_cb = constraint_cb\n",
        "        self.stocasthic = stocasthic\n",
        "\n",
        "    def get_dataset(self, n_points):\n",
        "        if self.constraint_cb is None:\n",
        "            x = np.random.rand(n_points, self.input_shape)\n",
        "            for i, b in enumerate(self.input_bounds):\n",
        "                lb = b[0]\n",
        "                ub = b[1]\n",
        "                if self.input_type[i] == \"int\":\n",
        "                    x[:,i] = np.random.randint(lb, high=ub, size=n_points)\n",
        "                else:\n",
        "                    x[:,i] *= ub - lb\n",
        "                    x[:,i] += lb\n",
        "            y = np.zeros((n_points))\n",
        "            for i in range(n_points):\n",
        "                y[i] = self.fun(x[i, :])\n",
        "            return x, y\n",
        "\n",
        "        # Constrained dataset:\n",
        "        x = np.zeros((n_points, self.input_shape))\n",
        "        bounds = []\n",
        "        for i, b in enumerate(self.input_bounds):\n",
        "            if self.input_type[i] == \"int\":\n",
        "                bounds.append((int(b[0]), int(b[1])))\n",
        "            else:\n",
        "                bounds.append((float(b[0]), float(b[1])))\n",
        "\n",
        "        n_points_boundaries = n_points // 2\n",
        "        for p in range(n_points_boundaries):\n",
        "            cplex = cpx.Model()\n",
        "            xvars = []\n",
        "            for i,b in enumerate(self.input_bounds):\n",
        "                if self.input_type[i] == \"int\":\n",
        "                    xvars.append(cplex.integer_var(lb=b[0], ub=b[1], name=\"x\"+str(i)))\n",
        "                else:\n",
        "                    xvars.append(cplex.continuous_var(lb=b[0], ub=b[1], name=\"x\"+str(i)))\n",
        "            \n",
        "            csts = self.constraint_cb(cplex, xvars)\n",
        "            for pc in csts:\n",
        "                cplex.add_constraint(*pc)\n",
        "\n",
        "            # linear random objective (boundaries)\n",
        "            obj = 0\n",
        "            for var in xvars:\n",
        "                obj += var * (np.random.uniform()*2-1)\n",
        "            cplex.set_objective(\"max\", obj)\n",
        "            \n",
        "            # solve\n",
        "            cplex.set_time_limit(30)\n",
        "            sol = cplex.solve()\n",
        "            for i in range(self.input_shape):\n",
        "                x[p, i] = sol[\"x\"+str(i)]\n",
        "\n",
        "        # Interpolation  \n",
        "        for p in range(n_points_boundaries, n_points):\n",
        "            pidx = np.random.choice(n_points_boundaries, 2, replace=False)\n",
        "            p0, p1 = x[pidx[0]], x[pidx[1]]\n",
        "            step = np.random.uniform()\n",
        "            df = pd.DataFrame([p0, [np.nan]*self.input_shape, p1], \n",
        "                              index=[0, step, 1])\n",
        "            df = df.interpolate(method=\"index\")\n",
        "            x[p] = df.iloc[1].values\n",
        "\n",
        "        # eval fun\n",
        "        y = np.zeros((n_points))\n",
        "        for i in range(n_points):\n",
        "            y[i] = self.fun(x[i, :])\n",
        "\n",
        "        return x,y\n",
        "\n",
        "    def get_grid(self, n_points):\n",
        "        x_list = []\n",
        "        for i, b in enumerate(self.input_bounds):\n",
        "            lb = b[0]\n",
        "            ub = b[1]\n",
        "            if self.input_type[i] == \"int\":\n",
        "                x_list.append(np.arange(lb, ub, max(1, (ub-lb)//n_points)))\n",
        "            else:\n",
        "                x_list.append(np.arange(lb, ub, (ub-lb)/n_points))\n",
        "        x = np.array(np.meshgrid(*x_list)).reshape(self.input_shape,-1).T\n",
        "        y = np.zeros((x.shape[0]))\n",
        "        for i in range(x.shape[0]):\n",
        "            y[i] = self.fun(x[i, :])\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f97wo6u7pHxT"
      },
      "source": [
        "##TFP utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "Cj1QIgqNpSi3"
      },
      "outputs": [],
      "source": [
        "#@title Build and plot\n",
        "\n",
        "from matplotlib import cm\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "def build_probabilistic_regressor(input_shape, depth=4, width=20):\n",
        "    mdl = tf.keras.Sequential()\n",
        "    mdl.add(tf.keras.layers.Input(shape=(input_shape,), dtype='float32'))\n",
        "    for i in range(depth):\n",
        "        mdl.add(tf.keras.layers.Dense(width, activation='relu'))\n",
        "    mdl.add(tf.keras.layers.Dense(2, activation='linear'))\n",
        "    lf = lambda t: tfp.distributions.Normal(loc=t[:, :1], \n",
        "                                            scale=tf.keras.backend.exp(t[:, 1:]))\n",
        "    mdl.add(tfp.layers.DistributionLambda(lf))\n",
        "    return mdl\n",
        "\n",
        "def dlambda_likelihood(y_true, dist):\n",
        "    return -dist.log_prob(y_true)\n",
        "\n",
        "def plot_prob_predictions(mdl, x, y):\n",
        "    prob_pred = mdl(np.expand_dims(x, axis=1))\n",
        "    pred = prob_pred.mean().numpy().ravel()\n",
        "    std_pred = prob_pred.stddev().numpy().ravel()\n",
        "    plt.plot(x, y, c=\"grey\")\n",
        "    plt.plot(x, pred)\n",
        "    plt.fill_between(x, pred-std_pred, pred+std_pred, \n",
        "                    alpha=0.3, color='tab:blue', label='+/- std')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txjrAWgNO1RE"
      },
      "source": [
        "## Bayesian optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "j9z-F2az2mtZ"
      },
      "outputs": [],
      "source": [
        "#@title GPy opt - LCB\n",
        "\n",
        "import GPy\n",
        "from GPyOpt.methods import BayesianOptimization\n",
        "\n",
        "def bayesian_opt_gpy(problem, iterations, starting_points):\n",
        "    set_seed()\n",
        "\n",
        "    x,y = problem.get_dataset(starting_points)\n",
        "    if problem.input_shape == 1:\n",
        "        x = x.reshape(-1, 1)\n",
        "\n",
        "    bds = []\n",
        "    for i,b in enumerate(problem.input_bounds):\n",
        "        bds.append({\n",
        "            'name': 'X'+str(i), \n",
        "            'type': 'continuous', \n",
        "            'domain': problem.input_bounds[i]})\n",
        "        \n",
        "    def wrapper(in_x):\n",
        "        return problem.fun(in_x.ravel())\n",
        "\n",
        "    kernel = GPy.kern.Matern52(input_dim=1, variance=1.0, lengthscale=1.0)\n",
        "    optimizer = BayesianOptimization(f=wrapper, \n",
        "                                    domain=bds,\n",
        "                                    model_type='GP',\n",
        "                                    kernel=kernel,\n",
        "                                    acquisition_type ='EI',\n",
        "                                    acquisition_jitter = 0.01,\n",
        "                                    X=x,\n",
        "                                    Y=y.reshape(-1,1),\n",
        "                                    exact_feval=False,\n",
        "                                    normalize_Y=True,\n",
        "                                    maximize=False)\n",
        "\n",
        "    optimizer.run_optimization(max_iter=iterations)\n",
        "    optimizer.plot_acquisition()\n",
        "    print(\"Min found:\", optimizer.x_opt, optimizer.fx_opt)\n",
        "    if ENABLE_WANDB:\n",
        "        wandb.log({\n",
        "            \"min_found\": np.min(optimizer.get_evaluations()[1]),\n",
        "            \"n_iterations\": np.argmin(optimizer.get_evaluations()[1])+1-starting_points\n",
        "        })\n",
        "    return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWebt0aFQFWB"
      },
      "source": [
        "## EML utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "eKQuuVzEAFdY"
      },
      "outputs": [],
      "source": [
        "#@title Parse TFP model\n",
        "\n",
        "from eml.net.reader import keras_reader\n",
        "\n",
        "def parse_tfp(model):\n",
        "    in_shape = model.input_shape[1]\n",
        "    mdl_no_dist = tf.keras.Sequential()\n",
        "    mdl_no_dist.add(tf.keras.layers.Input(shape=(in_shape,), dtype='float32'))\n",
        "    for i in range(len(model.layers)-2):\n",
        "        w = model.layers[i].weights[1].shape[0]\n",
        "        mdl_no_dist.add(tf.keras.layers.Dense(w, activation='relu'))\n",
        "    mdl_no_dist.add(tf.keras.layers.Dense(2, activation='linear'))\n",
        "\n",
        "    mdl_no_dist.set_weights(model.get_weights())\n",
        "\n",
        "    nn = keras_reader.read_keras_sequential(mdl_no_dist)\n",
        "    return nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "d6sGBt_pBSDV"
      },
      "outputs": [],
      "source": [
        "#@title Bounds\n",
        "\n",
        "from eml.net.process import ibr_bounds, fwd_bound_tighthening\n",
        "\n",
        "def propagate_bound(bkd, parsed_model, bounds):\n",
        "    bounds = np.array(bounds)\n",
        "    parsed_model.layer(0).update_lb(bounds[:,0])\n",
        "    parsed_model.layer(0).update_ub(bounds[:,1])\n",
        "    fwd_bound_tighthening(bkd, parsed_model, timelimit=30)\n",
        "    print(\"Model output bounds:\\n\",parsed_model)\n",
        "    return parsed_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "YaQm5IIOJKKh"
      },
      "outputs": [],
      "source": [
        "#@title Encode model\n",
        "\n",
        "from eml.backend import cplex_backend\n",
        "from eml.net import embed\n",
        "\n",
        "def embed_model(bkd, cplex, parsed_model, vtype, bounds):\n",
        "    mean_lb = parsed_model.layer(-1).lb()[0]  # bounds computed with propagate bounds method\n",
        "    mean_ub = parsed_model.layer(-1).ub()[0]\n",
        "    std_lb = parsed_model.layer(-1).lb()[1]\n",
        "    std_ub = parsed_model.layer(-1).ub()[1]\n",
        "\n",
        "    xvars = []\n",
        "    for i,b in enumerate(bounds):\n",
        "        xvars.append(cplex.continuous_var(lb=b[0], ub=b[1], name=\"x\"+str(i)))\n",
        "\n",
        "    yvars = [cplex.continuous_var(lb=mean_lb, ub=mean_ub, name=\"out_mean\"), \n",
        "            cplex.continuous_var(lb=std_lb, ub=std_ub, name=\"out_std\")]\n",
        "\n",
        "    embed.encode(bkd, parsed_model, cplex, xvars, yvars, 'nn')\n",
        "    return xvars, yvars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "Oacl6WbaqGkm"
      },
      "outputs": [],
      "source": [
        "#@title PWL helper\n",
        "\n",
        "from eml.util import encode_pwl\n",
        "from scipy.stats import norm\n",
        "\n",
        "\n",
        "def pwl_exp(bkd, cplex, var, nnodes=7):\n",
        "    xx = np.linspace(var.lb, var.ub, nnodes)\n",
        "    yy = np.array(list(map(math.exp, xx)))\n",
        "    v = [cplex.continuous_var(lb=0, ub=np.max(yy), name=\"exp_out\"), \n",
        "         var]\n",
        "    encode_pwl(bkd, cplex, v, [yy,xx])\n",
        "    return v[0]\n",
        "\n",
        "def pwl_abs(bkd, cplex, var):\n",
        "    xx = np.array([-1, 0, 1])\n",
        "    yy = np.array([1, 0, 1])\n",
        "    v = [cplex.continuous_var(lb=0, ub=1), \n",
        "         var]\n",
        "    encode_pwl(bkd, cplex, v, [yy,xx])\n",
        "    return v[0]\n",
        "\n",
        "def pwl_normal_cdf(bkd, cplex, var, nnodes=11):\n",
        "    xx = np.linspace(var.lb, var.ub, nnodes)\n",
        "    yy = np.array(list(map(norm.cdf, xx)))\n",
        "    v = [cplex.continuous_var(lb=0, ub=1, name=\"ncdf_out\"), \n",
        "         var]\n",
        "    encode_pwl(bkd, cplex, v, [yy,xx])\n",
        "    return v[0]\n",
        "\n",
        "def pwl_normal_pdf(bkd, cplex, var, nnodes=11):\n",
        "    xx = np.linspace(var.lb, var.ub, nnodes)\n",
        "    yy = np.array(list(map(norm.pdf, xx)))\n",
        "    v = [cplex.continuous_var(lb=0, ub=1, name=\"npdf_out\"), \n",
        "         var]\n",
        "    encode_pwl(bkd, cplex, v, [yy,xx])\n",
        "    return v[0]\n",
        "\n",
        "def pwl_sample_dist(bkd, cplex, vars, samples, vtype, bounds, nnodes=20):\n",
        "    mapf = lambda x: np.min(np.sum(np.abs(samples - x), axis=1)) / len(bounds)\n",
        "    p = Problem(None, mapf, vtype, bounds)\n",
        "    x,y = p.get_grid(nnodes)\n",
        "    ub = np.diff(np.array(bounds), axis=1)\n",
        "    ub = np.sum(ub)\n",
        "    v = [cplex.continuous_var(lb=0, ub=ub, name=\"dist_out\")]+vars\n",
        "    encode_pwl(bkd, cplex, v, [y,*x.T])\n",
        "    return v[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xe2oK4x9eI3W"
      },
      "source": [
        "## 🔄 Optimization loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "xC_4qcVo7u5Y"
      },
      "outputs": [],
      "source": [
        "#@title Base Experiment\n",
        "\n",
        "import time\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "class BaseExperiment:\n",
        "    def __init__(self, problem, starting_points, iterations, \n",
        "                 epochs, lr, weight_decay, depth, width, batch_size, \n",
        "                 solver_timeout):\n",
        "        set_seed()\n",
        "        self.problem = problem\n",
        "        self.starting_points = starting_points\n",
        "        self.iterations = iterations\n",
        "        self.epochs = epochs\n",
        "        self.lr = lr\n",
        "        self.weight_decay = weight_decay\n",
        "        self.depth = depth\n",
        "        self.width = width\n",
        "        self.batch_size = batch_size\n",
        "        self.solver_timeout = solver_timeout\n",
        "        self.x_samples, self.y_samples = problem.get_dataset(starting_points)\n",
        "\n",
        "    def train(self, keras_mdl):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def solver_optimization(self, keras_mdl):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def solution_log(self, solution):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def normalize_X(self, x):\n",
        "        bounds = np.array(self.problem.input_bounds)\n",
        "        return (x-bounds[:,0]) / (bounds[:,1]-bounds[:,0])\n",
        "\n",
        "    def normalize_Y(self, y):\n",
        "        min, max = np.min(self.y_samples), np.max(self.y_samples)\n",
        "        return (y-min) / (max-min)\n",
        "\n",
        "    def reverse_normalize_X(self, norm_x, milp_expr=False):\n",
        "        bounds = np.array(self.problem.input_bounds)\n",
        "        if milp_expr:  \n",
        "            # for cplex variables, np array not supported\n",
        "            bounds_range = np.squeeze(np.diff(bounds, axis=1))\n",
        "            xvars = []\n",
        "            for i,x in enumerate(norm_x):\n",
        "                xvars.append(x * bounds_range[i] + bounds[i,0])\n",
        "            return xvars\n",
        "        return (bounds[:,1]-bounds[:,0])*norm_x + bounds[:,0]\n",
        "\n",
        "    def reverse_normalize_Y(self, norm_y, stddev=False):\n",
        "        min, max = np.min(self.y_samples), np.max(self.y_samples)\n",
        "        if stddev:\n",
        "            return norm_y*(max-min)\n",
        "        return (max-min)*norm_y + min\n",
        "\n",
        "    def plot(self, hstory, keras_mdl):\n",
        "        if self.problem.input_shape <= 2:\n",
        "            x,y = self.problem.get_grid(100)\n",
        "            prob_pred = keras_mdl(self.normalize_X(x))\n",
        "            pred = prob_pred.mean().numpy().ravel()\n",
        "            pred = self.reverse_normalize_Y(pred)\n",
        "            std_pred = prob_pred.stddev().numpy().ravel()\n",
        "            std_pred = self.reverse_normalize_Y(std_pred, stddev=True)\n",
        "\n",
        "        plt.plot(hstory.history[\"loss\"])\n",
        "        plt.savefig('train_loss.png')\n",
        "        plt.show()\n",
        "        \n",
        "        fig = plt.figure(figsize=(15,10))\n",
        "        if self.problem.input_shape == 1:   # 1D domain\n",
        "            plt.xlim(self.problem.input_bounds[0])\n",
        "            x = np.squeeze(x)\n",
        "            plt.plot(x, y, c=\"grey\")\n",
        "            plt.plot(x, pred)\n",
        "            plt.fill_between(x, pred-std_pred, pred+std_pred, \n",
        "                            alpha=0.3, color='tab:blue', label='+/- std')\n",
        "            plt.scatter(self.x_samples, self.y_samples, c=\"orange\")\n",
        "            plt.legend([\"GT\", \"predicted mean\", \"predicted CI\", \"samples\"])\n",
        "            plt.savefig('chart.png')\n",
        "            plt.show()\n",
        "        elif self.problem.input_shape == 2:   # 2D domain\n",
        "            ax = fig.add_subplot(111, projection='3d')\n",
        "            ax.scatter(self.x_samples[:,0], self.x_samples[:,1], self.y_samples, color=\"orange\")\n",
        "            ax.scatter(x[:,0], x[:,1], y, alpha=0.15, color=\"lightgrey\")\n",
        "            ax.scatter(x[:,0], x[:,1], pred, alpha=0.3)\n",
        "            ax.scatter(x[:,0], x[:,1], pred-std_pred, alpha=0.3, color=\"lightblue\")\n",
        "            ax.scatter(x[:,0], x[:,1], pred+std_pred, alpha=0.3, color=\"lightblue\")\n",
        "            ax.view_init(elev=15, azim=60)\n",
        "            plt.legend([\"samples\", \"GT\", \"predicted mean\", \"predicted CI\"])\n",
        "            plt.savefig('chart.png')\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(\"Plot not available for high dimensional domains.\")\n",
        "            print(f\"X:\\n{self.x_samples}\\nY:\\n{self.y_samples}\")\n",
        "            prob_pred = keras_mdl(self.normalize_X(self.x_samples))\n",
        "            mu = self.reverse_normalize_Y(prob_pred.mean().numpy().ravel())\n",
        "            print(\"mu pred\\n\",mu)\n",
        "            print(\"diff\\n\",self.y_samples-mu)\n",
        "\n",
        "    def run(self):\n",
        "        not_improve = 0\n",
        "        for iteration in range(self.iterations):\n",
        "            # build NN\n",
        "            print(f\"Iteration {iteration}:\", \"=\"*20)\n",
        "            optimizer = tfa.optimizers.AdamW(weight_decay=self.weight_decay,\n",
        "                                             learning_rate=self.lr)\n",
        "            keras_mdl = build_probabilistic_regressor(\n",
        "                self.problem.input_shape, self.depth, self.width)\n",
        "            keras_mdl.compile(optimizer=optimizer, loss=dlambda_likelihood)\n",
        "            keras_mdl.summary()\n",
        "\n",
        "            # train\n",
        "            train_time = time.time()\n",
        "            keras_mdl, hstory = self.train(keras_mdl)\n",
        "            train_time = time.time() - train_time\n",
        "\n",
        "            # plot loss and predictions if domain dim <= 2\n",
        "            self.plot(hstory, keras_mdl)\n",
        "\n",
        "            # MILP solver\n",
        "            solver_time = time.time()\n",
        "            sol = self.solver_optimization(iteration, not_improve, keras_mdl)\n",
        "            solver_time = time.time() - solver_time\n",
        "\n",
        "            if sol is None:\n",
        "                print(f'Not feasible')\n",
        "                break\n",
        "            sol.solve_details.print_information()\n",
        "\n",
        "            # retrieve solution\n",
        "            opt_x = np.zeros(self.problem.input_shape)\n",
        "            for i in range(self.problem.input_shape):\n",
        "                opt_x[i] = sol[\"x\"+str(i)]\n",
        "            opt_x = self.reverse_normalize_X(opt_x)\n",
        "            \n",
        "            # retrieve integer solutions\n",
        "            for i in range(self.problem.input_shape):\n",
        "                if self.problem.input_type[i] == \"int\":\n",
        "                    opt_x[i] = sol[\"int_x\"+str(i)]\n",
        "\n",
        "            # check if repeated data point and query obj function\n",
        "            dists = np.sum(np.abs(self.x_samples - np.expand_dims(opt_x, 0)), axis=1)\n",
        "            dmin = np.min(dists)\n",
        "            dargmin = np.argmin(dists)\n",
        "            # For non stocasthic problems, avoid resampling the same points, but giving much sample weight\n",
        "            if not self.problem.stocasthic and dmin <= 1e-6: \n",
        "                print(\"Preventing query for an already known point !\")\n",
        "                opt_y = self.y_samples[dargmin]\n",
        "            else:\n",
        "                # BBF query\n",
        "                opt_y = self.problem.fun(opt_x)\n",
        "\n",
        "            # log new point\n",
        "            emean = self.reverse_normalize_Y(sol['out_mean'])\n",
        "            print(f\"opt input={opt_x}, expected mean={emean}, gt={opt_y}\")\n",
        "            print(f\"train time: {train_time}, solver time: {solver_time}\")\n",
        "\n",
        "            # not improve counter\n",
        "            if opt_y < np.min(self.y_samples):\n",
        "                not_improve = 0\n",
        "            else:\n",
        "                not_improve += 1\n",
        "            \n",
        "            # add new point to the dataset\n",
        "            self.x_samples = np.concatenate((self.x_samples, np.expand_dims(opt_x, 0)))\n",
        "            self.y_samples = np.append(self.y_samples, opt_y)\n",
        "\n",
        "            # log and WandB\n",
        "            if iteration == 0 and ENABLE_WANDB: # log starting points before the first iteration\n",
        "                for j in range(self.starting_points):\n",
        "                    wandb.log({\"x_sample\": self.x_samples[j], \"y_sample\": self.y_samples[j]})\n",
        "                \n",
        "            self.solution_log(sol)\n",
        "\n",
        "            if ENABLE_WANDB: \n",
        "                if self.problem.input_shape <= 2:\n",
        "                    wandb.log({\"train_predictions\": wandb.Image('chart.png')}, commit=False)\n",
        "                wandb.log({\n",
        "                    \"train_loss\": wandb.Image('train_loss.png'),\n",
        "                    \"x_sample\": opt_x, \"y_sample\": opt_y, \"y_min\": np.min(self.y_samples),\n",
        "                    \"train_time\": train_time, \"solver_time\": solver_time\n",
        "                })\n",
        "        # == end opt loop\n",
        "        print(f\"Min found: {np.min(self.y_samples)} in {np.argmin(self.y_samples)+1-self.starting_points} iterations\")\n",
        "        if ENABLE_WANDB:\n",
        "            wandb.log({\n",
        "                \"min_found\": np.min(self.y_samples),\n",
        "                \"n_iterations\": np.argmin(self.y_samples)+1-self.starting_points\n",
        "            })\n",
        "\n",
        "        # Dump points for future studies\n",
        "        dump_points = list(zip(self.x_samples, self.y_samples))\n",
        "        with open('points.pkl', 'wb') as f:\n",
        "            pickle.dump(dump_points, f)\n",
        "        if ENABLE_WANDB:\n",
        "            artifact = wandb.Artifact('datapoints', type='points')\n",
        "            artifact.add_file(\"points.pkl\")\n",
        "            wandb.log_artifact(artifact)\n",
        "\n",
        "        ymin = np.min(self.y_samples)\n",
        "        yargmin = np.argmin(self.y_samples)\n",
        "        xmin = self.x_samples[yargmin]\n",
        "        return ymin, xmin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "sEFhpl6xi54I"
      },
      "outputs": [],
      "source": [
        "#@title Base UCB\n",
        "\n",
        "class BaseUCBExperiment(BaseExperiment):\n",
        "\n",
        "    def __init__(self, beta_ucb, *args, **kwargs):\n",
        "        super(BaseUCBExperiment, self).__init__(*args, **kwargs)\n",
        "        self.beta = beta_ucb\n",
        "\n",
        "    def solver_optimization(self, iteration, not_improve, keras_mdl):\n",
        "        print(\"UCB solver...\")\n",
        "        cplex = cpx.Model()\n",
        "        bkd = cplex_backend.CplexBackend()\n",
        "\n",
        "        parsed_mdl = parse_tfp(keras_mdl)\n",
        "        propagate_bound(parsed_mdl, [[0,1]]*self.problem.input_shape)\n",
        "        xvars, yvars = embed_model(bkd, cplex, parsed_mdl, \n",
        "                                   self.problem.input_type,\n",
        "                                   [[0,1]]*self.problem.input_shape)\n",
        "\n",
        "        stddev = pwl_exp(bkd, cplex, yvars[1], nnodes=7)\n",
        "\n",
        "        # Problem contraints\n",
        "        if self.problem.constraint_cb is not None:\n",
        "            rev_norm_xvars = self.reverse_normalize_X(xvars, True)\n",
        "            self.problem.constraint_cb(cplex, rev_norm_xvars)\n",
        "        \n",
        "        ucb = -yvars[0] + self.beta * stddev\n",
        "        cplex.set_objective('max', ucb)\n",
        "        cplex.set_time_limit(self.solver_timeout)\n",
        "        sol = cplex.solve()\n",
        "\n",
        "        print(cplex.solve_details)\n",
        "        cplex.print_information()\n",
        "        return sol\n",
        "\n",
        "    def solution_log(self, solution):\n",
        "        print(f\"UCB: {solution.objective_value}\")\n",
        "        if ENABLE_WANDB: \n",
        "            wandb.log({\n",
        "                \"ucb\": solution['ucb'],\n",
        "                \"exp_err\": solution['exp_out'] - math.exp(solution['out_std'])\n",
        "            }, commit=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "3WyQS_G2ejql"
      },
      "outputs": [],
      "source": [
        "#@title EarlyStop\n",
        "\n",
        "class EarlyStop(BaseUCBExperiment):\n",
        "\n",
        "    def __init__(self, patience, eval_points, *args, **kwargs):\n",
        "        super(EarlyStop, self).__init__(*args, **kwargs)\n",
        "        self.eval_points = eval_points\n",
        "        self.x_val, self.y_val = self.problem.get_dataset(self.eval_points)\n",
        "        self.x_val = self.normalize_X(self.x_val)\n",
        "        self.y_val = self.normalize_Y(self.y_val)\n",
        "        self.cb = [tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", \n",
        "                                                    patience=patience, restore_best_weights=True)]\n",
        "        \n",
        "    def train(self, keras_mdl):\n",
        "        print(\"Early stop training...\")\n",
        "        bs = self.batch_size if self.batch_size else self.x_samples.shape[0]\n",
        "\n",
        "        norm_x = self.normalize_X(self.x_samples)\n",
        "        norm_y = self.normalize_Y(self.y_samples)\n",
        "\n",
        "        hstory = keras_mdl.fit(norm_x, norm_y, \n",
        "                               validation_data=(self.x_val, self.y_val),\n",
        "                               batch_size=bs, epochs=self.epochs, \n",
        "                               verbose=0, callbacks=self.cb)\n",
        "\n",
        "        return keras_mdl, hstory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "UqLsO46NeTmX"
      },
      "outputs": [],
      "source": [
        "#@title Stop CI\n",
        "\n",
        "class StopCICB(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def __init__(self, threshold, *args, **kwargs):\n",
        "        super(StopCICB, self).__init__(*args, **kwargs)\n",
        "        self.threshold = threshold\n",
        "        self.x = None\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        prob_pred = self.model(self.x)\n",
        "        std_pred = prob_pred.stddev().numpy().ravel()\n",
        "        if epoch % 100 == 0:\n",
        "            print(epoch, \"mean stddev\", np.mean(std_pred))\n",
        "        if np.mean(std_pred) < self.threshold:\n",
        "            print(\"break condition on epoch\", epoch, \"with mean stddev\", np.mean(std_pred))\n",
        "            self.model.stop_training = True\n",
        "\n",
        "class StopCI(BaseUCBExperiment):\n",
        "\n",
        "    def __init__(self, ci_threshold, *args, **kwargs):\n",
        "        super(StopCI, self).__init__(*args, **kwargs)\n",
        "        self.ci_threshold = ci_threshold\n",
        "        stop_ci = StopCICB(self.ci_threshold)\n",
        "        self.cb = [stop_ci]\n",
        "\n",
        "    def train(self, keras_mdl):\n",
        "        print(\"Stop CI training...\")\n",
        "        bs = self.batch_size if self.batch_size else self.x_samples.shape[0]\n",
        "\n",
        "        norm_x = self.normalize_X(self.x_samples)\n",
        "        norm_y = self.normalize_Y(self.y_samples)\n",
        "\n",
        "        self.cb[0].x = norm_x\n",
        "\n",
        "        hstory = keras_mdl.fit(norm_x, norm_y,\n",
        "                batch_size=bs, epochs=self.epochs, verbose=0, callbacks=self.cb)\n",
        "\n",
        "        return keras_mdl, hstory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "XtcmHVARg7PJ"
      },
      "outputs": [],
      "source": [
        "#@title AugmentUniform\n",
        "\n",
        "class AugmentUniform(StopCI):\n",
        "\n",
        "    def __init__(self, num_aug, *args, **kwargs):\n",
        "        super(AugmentUniform, self).__init__(*args, **kwargs)\n",
        "        self.num_aug = num_aug\n",
        "        self.x_aug_samples = None\n",
        "        self.y_aug_samples = None\n",
        "\n",
        "    def plot(self, hstory, keras_mdl):\n",
        "        if self.problem.input_shape <= 2:\n",
        "            x,y = self.problem.get_grid(100)\n",
        "            prob_pred = keras_mdl(self.normalize_X(x))\n",
        "            pred = prob_pred.mean().numpy().ravel()\n",
        "            pred = self.reverse_normalize_Y(pred)\n",
        "            std_pred = prob_pred.stddev().numpy().ravel()\n",
        "            std_pred = self.reverse_normalize_Y(std_pred, stddev=True)\n",
        "\n",
        "        plt.plot(hstory.history[\"loss\"])\n",
        "        plt.savefig('train_loss.png')\n",
        "        plt.show()\n",
        "\n",
        "        fig = plt.figure(figsize=(15,10))\n",
        "        if self.problem.input_shape == 1:   # 1D domain\n",
        "            plt.xlim(self.problem.input_bounds[0])\n",
        "            x = np.squeeze(x)\n",
        "            plt.plot(x, y, c=\"grey\")\n",
        "            plt.plot(x, pred)\n",
        "            plt.fill_between(x, pred-std_pred, pred+std_pred, \n",
        "                            alpha=0.3, color='tab:blue', label='+/- std')\n",
        "            plt.scatter(self.x_aug_samples, self.y_aug_samples, c=\"grey\")\n",
        "            plt.scatter(self.x_samples, self.y_samples, c=\"orange\")\n",
        "            plt.legend([\"GT\", \"predicted mean\", \"predicted CI\", \"samples\", \"augmented samples\"])\n",
        "            plt.savefig('chart.png')\n",
        "            plt.show()\n",
        "        elif self.problem.input_shape == 2:    # 2D domain\n",
        "            ax = fig.add_subplot(111, projection='3d')\n",
        "            ax.scatter(x[:,0], x[:,1], y, alpha=0.15, color=\"lightgrey\")\n",
        "            ax.scatter(self.x_aug_samples[:,0], self.x_aug_samples[:,1], self.y_aug_samples, c=\"grey\", alpha=0.3)\n",
        "            ax.scatter(self.x_samples[:,0], self.x_samples[:,1], self.y_samples, color=\"orange\", alpha=1)\n",
        "            ax.scatter(x[:,0], x[:,1], pred)\n",
        "            ax.scatter(x[:,0], x[:,1], pred-std_pred, alpha=0.3, color=\"lightblue\")\n",
        "            ax.scatter(x[:,0], x[:,1], pred+std_pred, alpha=0.3, color=\"lightblue\")\n",
        "            ax.view_init(elev=15, azim=60)\n",
        "            plt.legend([\"samples\", \"augmented samples\", \"predicted mean\", \"predicted CI\"])\n",
        "            plt.savefig('chart.png')\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(\"Plot not available for high dimensional domains.\")\n",
        "            print(f\"X:\\n{self.x_samples}\\nY:\\n{self.y_samples}\")\n",
        "\n",
        "    def train(self, keras_mdl):\n",
        "        print(\"Uniform augmented training...\")\n",
        "        self.x_aug_samples = np.concatenate((self.x_samples, \n",
        "                                        self.problem.get_dataset(self.num_aug)[0]))\n",
        "        y_range = np.max(np.abs(self.y_samples))*2                              \n",
        "        self.y_aug_samples = np.concatenate((self.y_samples, \n",
        "                                        (np.random.rand(self.num_aug)-0.5)*y_range))  \n",
        "\n",
        "        sw = np.ones_like(self.x_aug_samples)  \n",
        "        sw[:self.x_samples.shape[0]] = self.num_aug*2\n",
        "\n",
        "        bs = self.batch_size if self.batch_size else self.x_aug_samples.shape[0]\n",
        "\n",
        "        norm_x = self.normalize_X(self.x_aug_samples)\n",
        "        norm_y = self.normalize_Y(self.y_aug_samples)\n",
        "        self.cb[0].x = norm_x\n",
        "\n",
        "        hstory = keras_mdl.fit(norm_x, norm_y,\n",
        "            batch_size=bs, epochs=self.epochs, verbose=0, callbacks=self.cb, sample_weight=sw)\n",
        "        \n",
        "        return keras_mdl, hstory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "z62pqvO7hmhU"
      },
      "outputs": [],
      "source": [
        "#@title LHS\n",
        "\n",
        "class LHS(AugmentUniform):\n",
        "\n",
        "    def train(self, keras_mdl):\n",
        "        if self.problem.constraint_cb is not None:\n",
        "            raise Error(\"Contrained objective not supported by augmented LHS.\")\n",
        "            \n",
        "        print(\"LHS augmented training\")\n",
        "        n_datapoint = self.num_aug ** self.problem.input_shape\n",
        "\n",
        "        aug_x = self.problem.get_grid(self.num_aug)[0]\n",
        "        aug_x = np.concatenate((aug_x, aug_x))\n",
        "\n",
        "        y_range = np.max(np.abs(self.y_samples))*2                              \n",
        "        aug_y = np.concatenate(((np.random.rand(n_datapoint)-0.5)*y_range,\n",
        "                                (np.random.rand(n_datapoint)-0.5)*y_range))\n",
        "        self.x_aug_samples = np.concatenate((self.x_samples, aug_x))\n",
        "        self.y_aug_samples = np.concatenate((self.y_samples, aug_y))\n",
        "\n",
        "        sw = np.ones_like(self.x_aug_samples)  \n",
        "        sw[:self.x_samples.shape[0]] = n_datapoint\n",
        "\n",
        "        bs = self.batch_size if self.batch_size else self.x_aug_samples.shape[0]\n",
        "\n",
        "        norm_x = self.normalize_X(self.x_aug_samples)\n",
        "        norm_y = self.normalize_Y(self.y_aug_samples)\n",
        "        self.cb[0].x = norm_x\n",
        "\n",
        "        hstory = keras_mdl.fit(norm_x, norm_y,\n",
        "            batch_size=bs, epochs=self.epochs, verbose=0, callbacks=self.cb, sample_weight=sw)\n",
        "        \n",
        "        return keras_mdl, hstory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "k9nJMUn8mox3"
      },
      "outputs": [],
      "source": [
        "#@title MILP dist\n",
        "\n",
        "class PWLMILPDist(StopCI):\n",
        "\n",
        "    def solver_optimization(self, iteration, not_improve, keras_mdl):\n",
        "        print(\"Distance based UCB solver...\")\n",
        "        cplex = cpx.Model()\n",
        "        bkd = cplex_backend.CplexBackend()\n",
        "\n",
        "        parsed_mdl = parse_tfp(keras_mdl)\n",
        "        propagate_bound(parsed_mdl, [[0,1]]*self.problem.input_shape)\n",
        "        xvars, yvars = embed_model(bkd, cplex, parsed_mdl, \n",
        "                                   self.problem.input_type,\n",
        "                                   [[0,1]]*self.problem.input_shape)\n",
        "\n",
        "        stddev = pwl_exp(bkd, cplex, yvars[1], nnodes=7)\n",
        "\n",
        "        norm_x = self.normalize_X(self.x_samples)\n",
        "        dist = pwl_sample_dist(bkd, cplex, xvars, norm_x, \n",
        "                               self.problem.input_type,\n",
        "                               [[0,1]]*self.problem.input_shape, \n",
        "                               nnodes = min(20, self.x_samples.shape[0]))\n",
        "\n",
        "        # Problem contraints\n",
        "        if self.problem.constraint_cb is not None:\n",
        "            rev_norm_xvars = self.reverse_normalize_X(xvars, True)\n",
        "            self.problem.constraint_cb(cplex, rev_norm_xvars)\n",
        "\n",
        "        ucb = -yvars[0] + self.beta * (stddev + dist)\n",
        "\n",
        "        cplex.set_objective('max', ucb)\n",
        "        cplex.set_time_limit(self.solver_timeout)\n",
        "        sol = cplex.solve()\n",
        "\n",
        "        cplex.print_information()\n",
        "        return sol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "Tv2QdDXOnwW5"
      },
      "outputs": [],
      "source": [
        "#@title Hybrid\n",
        "\n",
        "class Hybrid(PWLMILPDist, AugmentUniform):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "FAQnPOZmByXx"
      },
      "outputs": [],
      "source": [
        "#@title FastMILP dist\n",
        "\n",
        "class FastMILPDist(StopCI):\n",
        "\n",
        "    def solver_optimization(self, iteration, not_improve, keras_mdl):\n",
        "        print(\"Distance based UCB solver...\")\n",
        "        cplex = cpx.Model()\n",
        "        bkd = cplex_backend.CplexBackend()\n",
        "\n",
        "        parsed_mdl = parse_tfp(keras_mdl)\n",
        "        propagate_bound(bkd, parsed_mdl, [[0,1]]*self.problem.input_shape)\n",
        "        xvars, yvars = embed_model(bkd, cplex, parsed_mdl, \n",
        "                                   self.problem.input_type,\n",
        "                                   [[0,1]]*self.problem.input_shape)\n",
        "\n",
        "        stddev = pwl_exp(bkd, cplex, yvars[1], nnodes=7)\n",
        "\n",
        "        # Distance\n",
        "        norm_x = self.normalize_X(self.x_samples)\n",
        "        dist = cplex.continuous_var(lb=0, ub=1, name=\"dist\")\n",
        "        for row in range(norm_x.shape[0]):\n",
        "            sd = 0\n",
        "            for feature in range(norm_x.shape[1]):\n",
        "                #sd += (norm_x[row, feature] - xvars[feature]) * (norm_x[row, feature] - xvars[feature]) # NOTE: non-convex\n",
        "                sd += cplex.abs(norm_x[row, feature] - xvars[feature])\n",
        "            cplex.add_constraint(sd >= dist*self.problem.input_shape)  # scale down the l1 dist with dimensions\n",
        "\n",
        "        # Problem contraints\n",
        "        if self.problem.constraint_cb is not None:\n",
        "            rev_norm_xvars = self.reverse_normalize_X(xvars, True)\n",
        "            self.problem.constraint_cb(cplex, rev_norm_xvars)\n",
        "           \n",
        "        # UCB\n",
        "        ucb = -yvars[0] + self.beta * (stddev + dist)\n",
        "\n",
        "        cplex.set_objective('max', ucb)\n",
        "        cplex.set_time_limit(self.solver_timeout)\n",
        "        sol = cplex.solve()\n",
        "\n",
        "        cplex.print_information()\n",
        "\n",
        "        box = sys.stdout\n",
        "        sys.stdout = open('cplex_model.txt', 'w')\n",
        "        cplex.prettyprint()\n",
        "        sys.stdout.close()\n",
        "        sys.stdout = box\n",
        "\n",
        "        return sol\n",
        "\n",
        "    def solution_log(self, solution):\n",
        "        print(\"Normalized Dist:\", solution['dist'])\n",
        "        print(f\"UCB: {solution.objective_value}\")\n",
        "        print(\"mean:\", solution['out_mean'])\n",
        "        print(\"logstd:\", solution['out_std'])\n",
        "        print(\"truestd:\", math.exp(solution['out_std']))\n",
        "        print(\"stddev:\", solution['exp_out'])\n",
        "        if ENABLE_WANDB: \n",
        "            wandb.log({\n",
        "                \"ucb\": solution.objective_value,\n",
        "                \"norm_dist\": solution['dist'],\n",
        "                \"exp_err\": solution['exp_out'] - math.exp(solution['out_std'])\n",
        "            }, commit=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "iIUvAeW-hUwG"
      },
      "outputs": [],
      "source": [
        "#@title Beta decay\n",
        "\n",
        "class BetaDecay(StopCI):\n",
        "\n",
        "    def __init__(self, lns_fixed, *args, **kwargs):\n",
        "        super(BetaDecay, self).__init__(*args, **kwargs)\n",
        "        self.lns_fixed = lns_fixed\n",
        "\n",
        "    def solver_optimization(self, iteration, not_improve, keras_mdl):\n",
        "        print(\"Distance based UCB solver...\")\n",
        "\n",
        "        ## K-Lipschitz with L1 dist\n",
        "        k_lip = 0\n",
        "        qlist = []\n",
        "        for i in range(self.x_samples.shape[0]):\n",
        "            for j in range(i+1, self.x_samples.shape[0]):\n",
        "                delta_y = np.abs(self.y_samples[i]-self.y_samples[j])\n",
        "                delta_x = np.sum(np.abs(self.x_samples[i]-self.x_samples[j]))/self.problem.input_shape\n",
        "                if delta_x >= 1e-6: #eps to avoid inf\n",
        "                    k_lip = max(k_lip, delta_y/delta_x)\n",
        "                    qlist.append(delta_y/delta_x)\n",
        "        print(\"K-Lipschitz max:\", k_lip, \n",
        "              \"95 percentile:\", np.percentile(qlist, 95, interpolation='linear'))\n",
        "        \n",
        "        solution = None\n",
        "        feature_fixed = self.problem.input_shape - (self.problem.input_shape // self.lns_fixed)\n",
        "        for lns_i in range(self.lns_fixed):\n",
        "            print(\"@\"*20, \"LSN iteration:\", lns_i)\n",
        "            cplex = cpx.Model()\n",
        "            bkd = cplex_backend.CplexBackend()\n",
        "            norm_x = self.normalize_X(self.x_samples)\n",
        "\n",
        "            ## Beta\n",
        "            if self.beta is not None:\n",
        "                self.current_beta = self.beta\n",
        "            else:\n",
        "                self.current_beta = k_lip * ((1-iteration/ITERATIONS)**4)\n",
        "\n",
        "            ## LNS\n",
        "            best = norm_x[np.argmin(self.y_samples)]\n",
        "            selection = np.random.choice(self.problem.input_shape, feature_fixed, replace=False)\n",
        "            print(\"LNS Random selection\", selection, \"best:\", best)\n",
        "            pbounds = [[0,1]]*self.problem.input_shape\n",
        "            for v in selection:\n",
        "                pbounds[v] = [best[v], best[v]]\n",
        "            \n",
        "            ## NN encoding\n",
        "            parsed_mdl = parse_tfp(keras_mdl)\n",
        "            print(\"Propagating bounds...\")\n",
        "            propagate_bound(bkd, parsed_mdl, pbounds)\n",
        "            xvars, yvars = embed_model(bkd, cplex, parsed_mdl, \n",
        "                                    self.problem.input_type,\n",
        "                                    pbounds)\n",
        "\n",
        "            # Handle integer variables\n",
        "            for idx, bound in enumerate(self.problem.input_bounds):\n",
        "                if self.problem.input_type[idx] == \"int\":\n",
        "                    int_cst = cplex.integer_var(lb=bound[0], ub=bound[1], name=\"int_x\"+str(idx))\n",
        "                    cplex.add_constraint(int_cst == xvars[idx]*(bound[1]-bound[0])+bound[0])\n",
        "            \n",
        "            ## EXP PWL\n",
        "            stddev = pwl_exp(bkd, cplex, yvars[1], nnodes=7)\n",
        "\n",
        "            #################### Distance section\n",
        "            # Sample distance\n",
        "            sample_distance_list = []\n",
        "            for row in range(norm_x.shape[0]):\n",
        "                current_sample_dist = 0\n",
        "                for feature in range(norm_x.shape[1]):\n",
        "                    current_sample_dist += cplex.abs(norm_x[row, feature] - xvars[feature])\n",
        "                sample_distance_list.append(current_sample_dist)\n",
        "            \n",
        "            ## triangular inequality\n",
        "            def l1_dist(a,b): \n",
        "                return np.sum(np.abs(a-b))\n",
        "            for id_xy in range(len(sample_distance_list)):\n",
        "                s_xy = sample_distance_list[id_xy]\n",
        "                for id_xz in range(id_xy, len(sample_distance_list)):\n",
        "                    s_xz = sample_distance_list[id_xz]\n",
        "                    cplex.add_constraint(s_xy <= s_xz + l1_dist(norm_x[id_xy],norm_x[id_xz]), \"traingular inequality\")\n",
        "\n",
        "            # Min distance\n",
        "            min_dist = cplex.continuous_var(lb=0, ub=self.problem.input_shape, name=\"dist\")\n",
        "            for current_sample_dist in sample_distance_list:\n",
        "                cplex.add_constraint(current_sample_dist >= min_dist, \"min dist\")  # scale down the l1 dist with dimensions\n",
        "            #################### END Distance section\n",
        "            \n",
        "            # Problem contraints\n",
        "            if self.problem.constraint_cb is not None:\n",
        "                rev_norm_xvars = self.reverse_normalize_X(xvars, True)\n",
        "                csts = self.problem.constraint_cb(cplex, rev_norm_xvars)\n",
        "                for pc in csts:\n",
        "                    cplex.add_constraint(*pc)\n",
        "\n",
        "            # UCB\n",
        "            ucb = -yvars[0] + \\\n",
        "                self.current_beta * stddev + \\\n",
        "                (self.current_beta/self.problem.input_shape) * min_dist\n",
        "\n",
        "            cplex.set_objective('max', ucb)\n",
        "            cplex.set_time_limit(self.solver_timeout)\n",
        "\n",
        "            cplex.parameters.mip.strategy.nodeselect = 2\n",
        "\n",
        "            # CPLEX log\n",
        "            cplex.context.solver.verbose = 5\n",
        "            cplex.context.solver.log_output = True\n",
        "\n",
        "            cplex.print_information()\n",
        "            box = sys.stdout\n",
        "            sys.stdout = open('cplex_model.txt', 'w')\n",
        "            cplex.prettyprint()\n",
        "            sys.stdout.close()\n",
        "            sys.stdout = box\n",
        "\n",
        "            print(\"Solving...\")\n",
        "            sol = cplex.solve()\n",
        "\n",
        "            if sol is not None:\n",
        "                if solution is not None:\n",
        "                    if sol.objective_value > solution.objective_value:\n",
        "                        solution = sol\n",
        "                else:\n",
        "                    solution = sol\n",
        "\n",
        "        return solution\n",
        "\n",
        "    def solution_log(self, solution):\n",
        "        print(\"Beta:\", self.current_beta)\n",
        "        print(\"Normalized Dist:\", solution['dist'])\n",
        "        print(f\"UCB: {solution.objective_value}\")\n",
        "        print(\"mean:\", solution['out_mean'])\n",
        "        print(\"logstd:\", solution['out_std'])\n",
        "        print(\"truestd:\", math.exp(solution['out_std']))\n",
        "        print(\"stddev:\", solution['exp_out'])\n",
        "        if ENABLE_WANDB: \n",
        "            wandb.log({\n",
        "                \"ucb\": solution.objective_value,\n",
        "                \"norm_dist\": solution['dist'],\n",
        "                \"mean\": solution['out_mean'],\n",
        "                \"stddev\": solution['exp_out'],\n",
        "                \"exp_err\": solution['exp_out'] - math.exp(solution['out_std']),\n",
        "                \"beta_ucb\": self.current_beta\n",
        "            }, commit=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TPC"
      ],
      "metadata": {
        "id": "rLsS4l1SR7Rz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Quantization definitions\n",
        "\n",
        "import tensorflow_model_optimization as tfmot\n",
        "from tensorflow_model_optimization.python.core.quantization.keras.default_8bit import default_8bit_quantize_scheme, default_8bit_quantize_registry, default_8bit_quantizers\n",
        "from tensorflow_model_optimization.quantization.keras.quantizers import LastValueQuantizer, MovingAverageQuantizer\n",
        "from tensorflow_model_optimization.quantization.keras import quantize_annotate_layer as ql\n",
        "from tensorflow_model_optimization.quantization.keras import quantize_annotate_model, QuantizeConfig\n",
        "\n",
        "\n",
        "class ConvWeightsQuantizer(LastValueQuantizer):\n",
        "  \"\"\"Quantizer for handling weights in Conv2D/DepthwiseConv2D layers.\"\"\"\n",
        "  def __init__(self, bits):\n",
        "        super(ConvWeightsQuantizer, self).__init__(\n",
        "            num_bits=bits, per_axis=True, symmetric=True, narrow_range=True)\n",
        "  def build(self, tensor_shape, name, layer):\n",
        "    min_weight = layer.add_weight(\n",
        "        name + '_min',\n",
        "        shape=(tensor_shape[-1],),\n",
        "        initializer=tf.keras.initializers.Constant(-6.0),\n",
        "        trainable=False)\n",
        "    max_weight = layer.add_weight(\n",
        "        name + '_max',\n",
        "        shape=(tensor_shape[-1],),\n",
        "        initializer=tf.keras.initializers.Constant(6.0),\n",
        "        trainable=False)\n",
        "    return {'min_var': min_weight, 'max_var': max_weight}\n",
        "\n",
        "class QConf(default_8bit_quantize_registry.Default8BitQuantizeConfig):\n",
        "    def __init__(self, bits, conv, *args, **kwargs):\n",
        "        super(QConf, self).__init__(*args, **kwargs)\n",
        "        self.bits = bits\n",
        "        if conv:\n",
        "            self.weight_quantizer = ConvWeightsQuantizer(bits)\n",
        "        else:\n",
        "            self.weight_quantizer = LastValueQuantizer(\n",
        "                num_bits=self.bits, per_axis=False, symmetric=True, narrow_range=True)\n",
        "        self.activation_quantizer = MovingAverageQuantizer(\n",
        "            num_bits=self.bits, per_axis=False, symmetric=False, narrow_range=False)\n",
        "\n",
        "class QAct(default_8bit_quantize_registry.Default8BitActivationQuantizeConfig):\n",
        "    def __init__(self, bits, *args, **kwargs):\n",
        "        super(QAct, self).__init__(*args, **kwargs)\n",
        "        self.bits = bits\n",
        "\n",
        "    def get_output_quantizers(self, layer):\n",
        "        self._assert_activation_layer(layer)\n",
        "        if not hasattr(layer.activation, '__name__'):\n",
        "            raise ValueError('Activation {} not supported by '\n",
        "                            'Default8BitActivationQuantizeConfig.'.format(\n",
        "                                layer.activation))\n",
        "        if layer.activation.__name__ in ['relu', 'swish']:\n",
        "            return [MovingAverageQuantizer(\n",
        "            num_bits=self.bits, per_axis=False, symmetric=False, narrow_range=False)]\n",
        "        elif layer.activation.__name__ in ['linear', 'softmax', 'sigmoid', 'tanh']:\n",
        "            return []\n",
        "        raise ValueError('Activation {} not supported by '\n",
        "                        'Default8BitActivationQuantizeConfig.'.format(\n",
        "                            layer.activation))\n",
        "\n",
        "class QReg(default_8bit_quantize_registry.Default8BitQuantizeRegistry):\n",
        "    def __init__(self, bitlist, *args, **kwargs):\n",
        "        super(QReg, self).__init__(*args, **kwargs)\n",
        "        self.bitlist = bitlist\n",
        "        self.counter = -1\n",
        "\n",
        "    def get_quantize_config(self, layer):\n",
        "        self.counter += 1\n",
        "        quantize_info = self._get_quantize_info(layer.__class__)\n",
        "        if layer.name.startswith('activation'):\n",
        "            return QAct(self.bitlist[self.counter])\n",
        "        return QConf(self.bitlist[self.counter], \n",
        "                     layer.name.startswith('conv'),  # enable ConvWeightsQuantizer\n",
        "                     quantize_info.weight_attrs,\n",
        "                     quantize_info.activation_attrs,\n",
        "                     quantize_info.quantize_output)\n",
        "\n",
        "class QScheme(default_8bit_quantize_scheme.Default8BitQuantizeScheme):\n",
        "    def __init__(self, bitlist, *args, **kwargs):\n",
        "        super(QScheme, self).__init__(*args, **kwargs)\n",
        "        self.bitlist = bitlist\n",
        "\n",
        "    def get_quantize_registry(self):\n",
        "        return QReg(self.bitlist)"
      ],
      "metadata": {
        "id": "75yWylWgN75g",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW0oUNuGetvq"
      },
      "source": [
        "# 🚀 Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IywTUKNLocnX",
        "cellView": "code"
      },
      "outputs": [],
      "source": [
        "#@title 🎯 Targets\n",
        "\n",
        "# Functions\n",
        "################################################################################\n",
        "\n",
        "def fun_polynomial(x):\n",
        "    return 0.5*(0.1*math.pow(5*x-1, 4) - 0.4*math.pow(5*x-1, 3) + 0.5*(5*x-1))\n",
        "\n",
        "def fun_ackley(x):\n",
        "    x1,x2 = x[0]-1,x[1]-2   # center in 1,2 to remove the 0 bias of nn\n",
        "    return -20*math.exp(-0.2*math.sqrt(0.5*(x1*x1+x2*x2))) - \\\n",
        "        math.exp(0.5*(math.cos(2*math.pi*x1)+math.cos(2*math.pi*x2))) + \\\n",
        "        math.e + 20\n",
        "\n",
        "def build_keane_bump(dim):\n",
        "    def f(x):\n",
        "        v1=0\n",
        "        for i in range(dim):\n",
        "            v1 += math.pow(math.cos(x[i]),4)\n",
        "        v2=2\n",
        "        for i in range(dim):\n",
        "            v2 *= math.pow(math.cos(x[i]),2)\n",
        "        v3=1e-6 #zero div\n",
        "        for i in range(dim):\n",
        "            v3 += (i+1)*x[i]*x[i]\n",
        "        v3 = math.sqrt(v3)\n",
        "        return -abs((v1-v2)/v3)\n",
        "\n",
        "    vtypes = [\"real\"]*dim\n",
        "    bounds = [[0, 10]]*dim\n",
        "    return f, vtypes, bounds\n",
        "\n",
        "def build_ackley(dim):\n",
        "    def f(x):\n",
        "        v1=0\n",
        "        for i in range(dim):\n",
        "            v1 += x[i]*x[i]\n",
        "        v1 = math.exp(-0.2*math.sqrt(v1/dim))\n",
        "        v2=0\n",
        "        for i in range(dim):\n",
        "            v2 += math.cos(2*math.pi*x[i])\n",
        "        v2 = math.exp(v2/dim)\n",
        "        return -20*v1-v2+20+math.e\n",
        "\n",
        "    vtypes = [\"real\"]*dim\n",
        "    bounds = [[-5, 10]]*dim\n",
        "    return f, vtypes, bounds\n",
        "\n",
        "def fun_mccormick(x):\n",
        "    x1,x2 = x[0],x[1]\n",
        "    return math.sin(x1+x2) + math.pow(x1-x2,2)-1.5*x1+2.5*x2+1\n",
        "\n",
        "def build_rosenbrock(rosenbrock_dim):\n",
        "    def f(x):\n",
        "        y=0\n",
        "        for i in range(rosenbrock_dim-1):\n",
        "            y+= 100 * math.pow(x[i+1] - x[i]*x[i], 2) + math.pow(1-x[i], 2)\n",
        "        return y\n",
        "    vtypes = [\"real\"]*rosenbrock_dim\n",
        "    bounds = [[-2, 2]]*rosenbrock_dim\n",
        "    return f, vtypes, bounds\n",
        "\n",
        "def build_tpc():\n",
        "    cifar10 = tf.keras.datasets.cifar10\n",
        "    (train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "    dataset_mean = train_images.mean(axis=(0,1,2))\n",
        "    dataset_std = train_images.std(axis=(0,1,2))\n",
        "    train_images = (train_images - dataset_mean) / dataset_std\n",
        "    test_images = (test_images - dataset_mean) / dataset_std\n",
        "\n",
        "    !mkdir pretrained_resnet18\n",
        "    !mkdir pretrained_resnet18/variables\n",
        "    !wget https://api.wandb.ai/artifactsV2/gcp-us/veri/QXJ0aWZhY3Q6NTU2NTg0NjE=/d9d4d8f866df84014e528bb3c5617816 -O  pretrained_resnet18/variables/variables.data-00000-of-00001\n",
        "    !wget https://api.wandb.ai/artifactsV2/gcp-us/veri/QXJ0aWZhY3Q6NTU2NTg0NjE=/4901af0e55327757ca7d7380b353279f -O  pretrained_resnet18/variables/variables.index\n",
        "    !wget https://api.wandb.ai/artifactsV2/gcp-us/veri/QXJ0aWZhY3Q6NTU2NTg0NjE=/0a1a30ebb8498c7adaab17365283b563 -O  pretrained_resnet18/keras_metadata.pb\n",
        "    !wget https://api.wandb.ai/artifactsV2/gcp-us/veri/QXJ0aWZhY3Q6NTU2NTg0NjE=/7c8a4682f521bac78f8a89b70342675b -O  pretrained_resnet18/saved_model.pb\n",
        "\n",
        "    def tpc(x):\n",
        "        print(\"Query for x=\", x)\n",
        "        model = tf.keras.models.load_model(\"pretrained_resnet18\")\n",
        "        model = quantize_annotate_model(model)\n",
        "        q_aware_model = tfmot.quantization.keras.quantize_apply(model, QScheme(x))\n",
        "        q_aware_model.compile(optimizer=\"adam\",\n",
        "                        loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
        "        # q_aware_model.summary()\n",
        "\n",
        "        q_aware_model.fit(train_images, train_labels,\n",
        "                        batch_size=512, epochs=5, validation_split=0.1,\n",
        "                        callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", \n",
        "                            patience=5, restore_best_weights=True)])\n",
        "        \n",
        "        q_aware_model_loss, q_aware_model_accuracy = q_aware_model.evaluate(\n",
        "            test_images, test_labels, verbose=0)\n",
        "        \n",
        "        return -q_aware_model_accuracy\n",
        "    return tpc\n",
        "\n",
        "# Constraints \n",
        "################################################################################\n",
        "\n",
        "def cst_paper(cplex, xvars):\n",
        "    r = 5\n",
        "    acc1 = 0\n",
        "    for xi in xvars:\n",
        "        acc1 += xi*xi\n",
        "    acc2 = 0\n",
        "    for xi in xvars:\n",
        "        acc2 += xi\n",
        "    return [[acc1 <= r*r, \"center_dist\"], [acc2 <= 0, \"cst2\"]]\n",
        "\n",
        "def cst_paper2(cplex, xvars):\n",
        "    return [[sum(xvars) <= 7.5*len(xvars), 'sum']]\n",
        "\n",
        "def cst_tpc_size(cplex, xvars):\n",
        "    return [[sum(xvars) <= len(xvars)*2.5, 'cst_size']]\n",
        "\n",
        "def cst_disk(cplex, xvars):     # x1^2 + x2^2 < r^2\n",
        "    x1,x2 = xvars\n",
        "    center = [1,2]\n",
        "    r = 0.5\n",
        "    x1 -= center[0]\n",
        "    x2 -= center[1]\n",
        "    cplex.add_constraint(x1*x1 + x2*x2 <= r*r)\n",
        "\n",
        "def cst_sphere(cplex, xvars):     # x1^2 + x2^2 + x3^2 < r^2\n",
        "    x1,x2,x3 = xvars\n",
        "    center = [-1,0.5,0.6]\n",
        "    r = 1.2\n",
        "    x1 -= center[0]\n",
        "    x2 -= center[1]\n",
        "    x3 -= center[2]\n",
        "    cplex.add_constraint(x1*x1 + x2*x2 + x3*x3 <= r*r)\n",
        "\n",
        "def cst_leq(cplex, xvars):     # x1 < x2\n",
        "    x1,x2 = xvars\n",
        "    cplex.add_constraint(x1 <= x2)\n",
        "\n",
        "# Problems\n",
        "################################################################################\n",
        "\n",
        "PROBLEM_POLYNOMIAL = Problem(\"polynomial\", \n",
        "                             fun_polynomial, \n",
        "                             [\"real\"],\n",
        "                             [[0, 1]])\n",
        "\n",
        "PROBLEM_ACKLEY = Problem(\"ackley\", \n",
        "                         fun_ackley, \n",
        "                         [\"real\", \"real\"],\n",
        "                         [[-3, 3], [-3, 3]])\n",
        "\n",
        "PROBLEM_CST_ACKLEY10D = Problem(\"constrained_ackley10D\", \n",
        "                             *build_ackley(10),  \n",
        "                             cst_paper)\n",
        "\n",
        "PROBLEM_INT_ACKLEY = Problem(\"int_ackley\", \n",
        "                             fun_ackley, \n",
        "                             [\"int\", \"real\"],\n",
        "                             [[-3, 3], [-2, 2]],\n",
        "                             cst_leq)\n",
        "\n",
        "PROBLEM_MCCORMICK = Problem(\"mccormick\", \n",
        "                            fun_mccormick, \n",
        "                            [\"real\", \"real\"],\n",
        "                            [[-1.5, 4], [-3, 4]])\n",
        "\n",
        "PROBLEM_ROSENBROCK2D = Problem(\"rosenbrock2D\", build_rosenbrock(2)[0], \n",
        "                               [\"real\", \"real\"], [[-2, 2]]*2)\n",
        "\n",
        "PROBLEM_ROSENBROCK40D = Problem(\"rosenbrock40D\", *build_rosenbrock(40))\n",
        "\n",
        "PROBLEM_KEANE2D = Problem(\"keane2D\", *build_keane_bump(2))\n",
        "\n",
        "PROBLEM_TPC = ProblemQRS(\"tpc\", build_tpc(), [\"int\"]*41, [[2, 8]]*41, cst_tpc_size, stocasthic=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "U3k-S4lGbtIu"
      },
      "outputs": [],
      "source": [
        "#@title 📈 Parameters and WandB\n",
        "\n",
        "ITERATIONS =                                  50#@param {type:\"integer\"}\n",
        "STARTING_POINTS =                              5#@param {type:\"integer\"}\n",
        "\n",
        "EPOCHS =                                    999#@param {type:\"integer\"}  \n",
        "PATIENCE =                                  None  #@param {type:\"raw\"}\n",
        "\n",
        "EVAL_POINTS = None                                   #@param {type:\"raw\"}\n",
        "LR =                                        1e-3#@param {type:\"number\"}\n",
        "WEIGHT_DECAY =                                        1e-4#@param {type:\"number\"}\n",
        "BATCH_SIZE = None                               #@param {type:\"raw\"} None = all samples in 1 batch\n",
        "DEPTH =                                        1#@param {type:\"integer\"}\n",
        "WIDTH =                                       200#@param {type:\"integer\"}\n",
        "\n",
        "BETA_UCB =                                     1#@param {type:\"raw\"}\n",
        "\n",
        "AUG_POINTS =                                 None#@param {type:\"raw\"}\n",
        "\n",
        "SOLVER_TIMEOUT =                              120#@param {type:\"integer\"}\n",
        "\n",
        "CI_THRESHOLD =              0.05#@param {type:\"raw\"} \n",
        "LNS_FIXED =              1#@param {type:\"integer\"} \n",
        "\n",
        "# set if you plan to log on wandb\n",
        "ENABLE_WANDB = True                            #@param {type:\"boolean\"}        \n",
        "\n",
        "if ENABLE_WANDB and \"wandb\" not in sys.modules:\n",
        "    !pip install wandb > /dev/null\n",
        "    !wandb login\n",
        "    import wandb\n",
        "\n",
        "def init_wandb(experiment_name, run_id):\n",
        "    if run_id is not None: \n",
        "        wandb.init(project='eml', id=run_id, resume='allow')\n",
        "    else:\n",
        "        wandb.init(project='eml', name=experiment_name)\n",
        "\n",
        "        wandb.config.iterations = ITERATIONS\n",
        "        wandb.config.starting_points = STARTING_POINTS\n",
        "        wandb.config.epochs = EPOCHS\n",
        "        wandb.config.patience = PATIENCE\n",
        "        wandb.config.eval_points = EVAL_POINTS\n",
        "        wandb.config.lr = LR\n",
        "        wandb.config.weight_decay = WEIGHT_DECAY\n",
        "        wandb.config.batch_size = BATCH_SIZE\n",
        "        wandb.config.depth = DEPTH\n",
        "        wandb.config.width = WIDTH\n",
        "        wandb.config.beta_ucb = BETA_UCB\n",
        "        wandb.config.aug_points = AUG_POINTS\n",
        "        wandb.config.solver_timeout = SOLVER_TIMEOUT\n",
        "        wandb.config.ci_threshold = CI_THRESHOLD\n",
        "        wandb.config.lns_fixed = LNS_FIXED\n",
        "\n",
        "def run_experiment(name, target):\n",
        "    if name == \"BayesianOptimization\":\n",
        "        optimizer = bayesian_opt_gpy(target, ITERATIONS, STARTING_POINTS)\n",
        "        return optimizer\n",
        "        \n",
        "    if name == \"EarlyStop\":\n",
        "        instance = EarlyStop(\n",
        "            PATIENCE, EVAL_POINTS, \n",
        "            BETA_UCB, \n",
        "            target, STARTING_POINTS, ITERATIONS, \n",
        "            EPOCHS, LR, WEIGHT_DECAY, DEPTH, WIDTH, BATCH_SIZE, \n",
        "            SOLVER_TIMEOUT)\n",
        "        instance.run()\n",
        "        return instance\n",
        "        \n",
        "    if name == \"StopCI\":\n",
        "        instance = StopCI(\n",
        "            CI_THRESHOLD, \n",
        "            BETA_UCB, \n",
        "            target, STARTING_POINTS, ITERATIONS, \n",
        "            EPOCHS, LR, WEIGHT_DECAY, DEPTH, WIDTH, BATCH_SIZE, \n",
        "            SOLVER_TIMEOUT)\n",
        "        instance.run()\n",
        "        return instance\n",
        "        \n",
        "    if name == \"AugmentUniform\":\n",
        "        instance = AugmentUniform(\n",
        "            AUG_POINTS, \n",
        "            CI_THRESHOLD, \n",
        "            BETA_UCB, \n",
        "            target, STARTING_POINTS, ITERATIONS, \n",
        "            EPOCHS, LR, WEIGHT_DECAY, DEPTH, WIDTH, BATCH_SIZE, \n",
        "            SOLVER_TIMEOUT)\n",
        "        instance.run()\n",
        "        return instance\n",
        "        \n",
        "    if name == \"AugmentLHS\":\n",
        "        instance = LHS(\n",
        "            AUG_POINTS, \n",
        "            CI_THRESHOLD, \n",
        "            BETA_UCB, \n",
        "            target, STARTING_POINTS, ITERATIONS, \n",
        "            EPOCHS, LR, WEIGHT_DECAY, DEPTH, WIDTH, BATCH_SIZE, \n",
        "            SOLVER_TIMEOUT)\n",
        "        instance.run()\n",
        "        return instance\n",
        "        \n",
        "    if name == \"PWLMILPDist\":\n",
        "        instance = PWLMILPDist(\n",
        "            CI_THRESHOLD, \n",
        "            BETA_UCB, \n",
        "            target, STARTING_POINTS, ITERATIONS, \n",
        "            EPOCHS, LR, WEIGHT_DECAY, DEPTH, WIDTH, BATCH_SIZE, \n",
        "            SOLVER_TIMEOUT)\n",
        "        instance.run()\n",
        "        return instance\n",
        "        \n",
        "    if name == \"Hybrid\":\n",
        "        instance = Hybrid(\n",
        "            AUG_POINTS, \n",
        "            CI_THRESHOLD,\n",
        "            BETA_UCB, \n",
        "            target, STARTING_POINTS, ITERATIONS, \n",
        "            EPOCHS, LR, WEIGHT_DECAY, DEPTH, WIDTH, BATCH_SIZE, \n",
        "            SOLVER_TIMEOUT)\n",
        "        instance.run()\n",
        "        return instance\n",
        "\n",
        "    if name == \"FastMILPDist\":\n",
        "        instance = FastMILPDist(\n",
        "            CI_THRESHOLD, \n",
        "            BETA_UCB, \n",
        "            target, STARTING_POINTS, ITERATIONS, \n",
        "            EPOCHS, LR, WEIGHT_DECAY, DEPTH, WIDTH, BATCH_SIZE, \n",
        "            SOLVER_TIMEOUT)\n",
        "        instance.run()\n",
        "        return instance\n",
        "    \n",
        "    if name == \"BetaDecay\":\n",
        "        instance = BetaDecay(\n",
        "            LNS_FIXED,\n",
        "            CI_THRESHOLD, \n",
        "            BETA_UCB, \n",
        "            target, STARTING_POINTS, ITERATIONS, \n",
        "            EPOCHS, LR, WEIGHT_DECAY, DEPTH, WIDTH, BATCH_SIZE, \n",
        "            SOLVER_TIMEOUT)\n",
        "        instance.run()\n",
        "        return instance\n",
        "\n",
        "    raise AttributeError(\"invalid method\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCI6spRykVDi",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ▶️ Run experiment\n",
        "\n",
        "# set if starting a new run\n",
        "EXPERIMENT_NAME = \"ackley10_vanilla\"      #@param {type:\"string\"}\n",
        "# set to None if starting a new run\n",
        "RUN_ID = None                                   #@param {type:\"raw\"}\n",
        "\n",
        "if ENABLE_WANDB:\n",
        "    init_wandb(EXPERIMENT_NAME, RUN_ID)\n",
        "\n",
        "INSTANCE = None\n",
        "try:\n",
        "    INSTANCE = run_experiment(\"BetaDecay\", PROBLEM_CST_ACKLEY10D)\n",
        "finally:\n",
        "    if ENABLE_WANDB:\n",
        "        wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "init_wandb(\"rosen2d\", RUN_ID)\n",
        "INSTANCE = None\n",
        "try:\n",
        "    INSTANCE = run_experiment(\"BetaDecay\", PROBLEM_ROSENBROCK2D)\n",
        "finally:\n",
        "    if ENABLE_WANDB:\n",
        "        wandb.finish()\n",
        "\n",
        "init_wandb(\"kean2d\", RUN_ID)\n",
        "INSTANCE = None\n",
        "try:\n",
        "    INSTANCE = run_experiment(\"BetaDecay\", PROBLEM_KEANE2D)\n",
        "finally:\n",
        "    if ENABLE_WANDB:\n",
        "        wandb.finish()"
      ],
      "metadata": {
        "id": "AUKMzCNxLsyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2USqSnCBVDM"
      },
      "source": [
        "# 🧪 Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C00w0zpdczA4"
      },
      "source": [
        "## Problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZIHdcoXc6qR"
      },
      "outputs": [],
      "source": [
        "def test_problem():\n",
        "    def f0(x):\n",
        "        return 0.5*(0.1*math.pow(5*x-1, 4) - 0.4*math.pow(5*x-1, 3) + 0.5*(5*x-1))\n",
        "    p0 = Problem(None, f0, [[0, 1]])\n",
        "\n",
        "    def f1(x):\n",
        "        x1,x2 = x[0],x[1]\n",
        "        return 10 * math.sin(x1) * math.sin(x2) \n",
        "    p1 = Problem(None, f1, [[-1, 1], [-1, 1]])\n",
        "\n",
        "    def ackley_fun(x):\n",
        "        x1,x2 = x[0],x[1]\n",
        "        return -20*math.exp(-0.2*math.sqrt(0.5*(x1*x1+x2*x2))) - \\\n",
        "            math.exp(0.5*(math.cos(2*math.pi*x1)+math.cos(2*math.pi*x2))) + \\\n",
        "            math.e + 20\n",
        "    ackley_problem = Problem(None, ackley_fun, [[-3, 3], [-3, 3]])\n",
        "\n",
        "    print(\"f0 grid\")\n",
        "    g0 = p0.get_grid(10)\n",
        "    x0, y0 = g0[0], g0[1]\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot()\n",
        "    ax.scatter(x0, y0)\n",
        "    plt.show()\n",
        "\n",
        "    print(\"f1 grid\")\n",
        "    g1 = p1.get_grid(10)\n",
        "    x1, y1 = g1[0], g1[1]\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(projection='3d')\n",
        "    ax.scatter(x1[:,0], x1[:,1], y1)\n",
        "    plt.show()\n",
        "\n",
        "    print(\"ackley grid\")\n",
        "    g1 = ackley_problem.get_grid(100)\n",
        "    x1, y1 = g1[0], g1[1]\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(projection='3d')\n",
        "    ax.scatter(x1[:,0], x1[:,1], y1)\n",
        "    plt.show()\n",
        "\n",
        "    print(\"f0 dataset\")\n",
        "    d0 = p0.get_dataset(10)\n",
        "    x0, y0 = d0[0], d0[1]\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot()\n",
        "    ax.scatter(x0, y0)\n",
        "    plt.show()\n",
        "\n",
        "    print(\"f1 dataset\")\n",
        "    d1 = p1.get_dataset(100)\n",
        "    x1, y1 = d1[0], d1[1]\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(projection='3d')\n",
        "    ax.scatter(x1[:,0], x1[:,1], y1)\n",
        "    plt.show()\n",
        "\n",
        "    print(\"ackley dataset\")\n",
        "    d1 = ackley_problem.get_dataset(10000)\n",
        "    x1, y1 = d1[0], d1[1]\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(projection='3d')\n",
        "    ax.scatter(x1[:,0], x1[:,1], y1)\n",
        "    plt.show()\n",
        "    \n",
        "test_problem()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUbgkjkyrII_"
      },
      "source": [
        "##TFP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "nOEy-I0YLVfG"
      },
      "outputs": [],
      "source": [
        "#@title Whole train set\n",
        "\n",
        "def tfp_test1():\n",
        "    set_seed()\n",
        "\n",
        "    p = Problem(None, polynomial_fun, [[0, 1]])\n",
        "    x,y = p.get_dataset(100)\n",
        "    x, y = x.ravel(), y.ravel()\n",
        "    x_train, y_train = x[:80], y[:80]\n",
        "    x_val, y_val = x[80:], y[80:]\n",
        "\n",
        "    mdl_prob = build_probabilistic_regressor(1)\n",
        "    mdl_prob.summary()\n",
        "    cb = [tf.keras.callbacks.EarlyStopping(patience=100, restore_best_weights=True)]\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "\n",
        "    bs = x_train.shape[0]\n",
        "    mdl_prob.compile(optimizer=optimizer, loss=dlambda_likelihood)\n",
        "    hstory = mdl_prob.fit(x_train, y_train, validation_data=(x_val, y_val), \n",
        "            batch_size=bs, epochs=5000, verbose=1, callbacks=cb)\n",
        "\n",
        "    sorted = x_val.argsort()\n",
        "    plot_prob_predictions(mdl_prob, x_val[sorted], y_val[sorted])\n",
        "\n",
        "tfp_test1()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "hEFazkyhSjwD"
      },
      "outputs": [],
      "source": [
        "#@title Few points linear\n",
        "\n",
        "def tfp_test2():\n",
        "    set_seed()\n",
        "\n",
        "    x_samples = np.random.rand(5)\n",
        "    y_samples = x_samples\n",
        "\n",
        "    mdl_prob = build_probabilistic_regressor(1)\n",
        "    mdl_prob.summary()\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "    bs = x_samples.shape[0]\n",
        "    mdl_prob.compile(optimizer=optimizer, loss=dlambda_likelihood)\n",
        "    hstory = mdl_prob.fit(x_samples, y_samples,\n",
        "            batch_size=bs, epochs=1000, verbose=1)\n",
        "\n",
        "    sorted = x_samples.argsort()\n",
        "    plot_prob_predictions(mdl_prob, x_samples[sorted], y_samples[sorted])\n",
        "\n",
        "tfp_test2()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "NuesUNvAV9HX"
      },
      "outputs": [],
      "source": [
        "#@title Different variance\n",
        "\n",
        "def tfp_test3():\n",
        "    set_seed()\n",
        "\n",
        "    points = np.random.rand(50)\n",
        "    noise = np.random.rand(20) / 5\n",
        "    x_samples = points\n",
        "    y_samples = np.copy(x_samples)\n",
        "    sorted_idx = x_samples.argsort()\n",
        "    y_samples[sorted_idx[:20]] += noise\n",
        "\n",
        "    mdl_prob = build_probabilistic_regressor(1)\n",
        "    mdl_prob.summary()\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "    bs = x_samples.shape[0]\n",
        "    mdl_prob.compile(optimizer=optimizer, loss=dlambda_likelihood)\n",
        "    hstory = mdl_prob.fit(x_samples, y_samples,\n",
        "            batch_size=bs, epochs=1000, verbose=1)\n",
        "\n",
        "    sorted = x_samples.argsort()\n",
        "    plot_prob_predictions(mdl_prob, x_samples[sorted], y_samples[sorted])\n",
        "\n",
        "tfp_test3()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1WCq0t24LeC"
      },
      "source": [
        "## EML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "Sv7hOKdHgMlz"
      },
      "outputs": [],
      "source": [
        "#@title Minimize cost_fn\n",
        "\n",
        "def eml_test1():\n",
        "    #train\n",
        "    p = Problem(None, polynomial_fun, [[0, 1]])\n",
        "    x,y = p.get_dataset(100)\n",
        "    x, y = x.ravel(), y.ravel()\n",
        "    x_train, y_train = x[:80], y[:80]\n",
        "    x_val, y_val = x[80:], y[80:]\n",
        "    mdl_prob = build_probabilistic_regressor(1)\n",
        "    mdl_prob.summary()\n",
        "    cb = [tf.keras.callbacks.EarlyStopping(patience=100, restore_best_weights=True)]\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "    bs = x_train.shape[0]\n",
        "    mdl_prob.compile(optimizer=optimizer, loss=dlambda_likelihood)\n",
        "    hstory = mdl_prob.fit(x_train, y_train, validation_data=(x_val, y_val), \n",
        "            batch_size=bs, epochs=5000, verbose=0, callbacks=cb)\n",
        "    \n",
        "    #solve\n",
        "    cplex = cpx.Model()\n",
        "    bkd = cplex_backend.CplexBackend()\n",
        "\n",
        "    parsed_mdl = parse_tfp(mdl_prob)\n",
        "    propagate_bound(parsed_mdl, [[0,1]])\n",
        "    xvars, yvars = embed_model(bkd, cplex, parsed_mdl, [[0,1]])\n",
        "\n",
        "    cplex.set_objective('min', yvars[0])\n",
        "    cplex.set_time_limit(30)\n",
        "\n",
        "    sol = cplex.solve()\n",
        "\n",
        "    print(f'feasible: {sol is not None}')\n",
        "    print(cplex_backend.model_to_string(cplex))\n",
        "    print(sol.display())\n",
        "\n",
        "    opt_x = sol.get_value(\"x0\")\n",
        "    opt_mean = sol.get_value(\"out_mean\")\n",
        "    opt_std = math.pow(math.e, sol.get_value(\"out_std\"))\n",
        "    print(opt_x, opt_mean, opt_std)\n",
        "\n",
        "    dist = mdl_prob(np.array([[opt_x]]))\n",
        "    assert dist.mean() - opt_mean < 1e-6\n",
        "    assert dist.stddev() - opt_std < 1e-6\n",
        "\n",
        "eml_test1()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "DXFInxIfr-6F"
      },
      "outputs": [],
      "source": [
        "#@title Test pwl\n",
        "\n",
        "def eml_test2():\n",
        "    cplex = cpx.Model()\n",
        "    bkd = cplex_backend.CplexBackend()\n",
        "\n",
        "    inp = cplex.continuous_var(lb=0, ub=1, name=\"test_in\")\n",
        "    out = pwl_normal_pdf(bkd, cplex, inp, nnodes=11)\n",
        "    cplex.set_objective('max', out)\n",
        "    cplex.set_time_limit(30)\n",
        "\n",
        "    cplex.prettyprint()\n",
        "    sol = cplex.solve()\n",
        "    print(f'feasible: {sol is not None}')\n",
        "    print(sol)\n",
        "\n",
        "    assert sol.get_value(\"test_in\") == 0\n",
        "\n",
        "eml_test2()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "jinmrHST7m0G"
      },
      "outputs": [],
      "source": [
        "#@title Minimize EI (non convex error)\n",
        "\n",
        "#   imp = mu - opt_mean - 0.1\n",
        "#   Z = imp / var\n",
        "#   ei = imp * norm.cdf(Z) + var * norm.pdf(Z)\n",
        "\n",
        "def eml_test3():\n",
        "    #train\n",
        "    p = Problem(None, polynomial_fun, [[0, 1]])\n",
        "    x,y = p.get_dataset(100)\n",
        "    x, y = x.ravel(), y.ravel()\n",
        "    x_train, y_train = x[:80], y[:80]\n",
        "    x_val, y_val = x[80:], y[80:]\n",
        "    mdl_prob = build_probabilistic_regressor(1)\n",
        "    mdl_prob.summary()\n",
        "    cb = [tf.keras.callbacks.EarlyStopping(patience=100, restore_best_weights=True)]\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "    bs = x_train.shape[0]\n",
        "    mdl_prob.compile(optimizer=optimizer, loss=dlambda_likelihood)\n",
        "    hstory = mdl_prob.fit(x_train, y_train, validation_data=(x_val, y_val), \n",
        "            batch_size=bs, epochs=5000, verbose=0, callbacks=cb)\n",
        "    \n",
        "    #solve\n",
        "    epsilon = 0.1\n",
        "    cplex = cpx.Model()\n",
        "    bkd = cplex_backend.CplexBackend()\n",
        "\n",
        "    parsed_mdl = parse_tfp(mdl_prob)\n",
        "    propagate_bound(parsed_mdl)\n",
        "    xvars, yvars = embed_model(bkd, cplex, parsed_mdl)\n",
        "\n",
        "    # EI\n",
        "    # find min mean (max improvement)\n",
        "    cplex.set_objective('min', yvars[0])\n",
        "    cplex.set_time_limit(30)\n",
        "    sol = cplex.solve()\n",
        "    print(f'feasible: {sol is not None}')\n",
        "    if sol:\n",
        "        opt_x = sol.get_value(\"in\")\n",
        "        opt_mean = sol.get_value(\"out_mean\")\n",
        "        opt_std = math.pow(math.e, sol.get_value(\"out_std\"))\n",
        "        print(opt_x, opt_mean, opt_std)\n",
        "\n",
        "    # compute ei\n",
        "    epsilon = 0.1\n",
        "    cplex = cpx.Model()\n",
        "    bkd = cplex_backend.CplexBackend()\n",
        "\n",
        "    parsed_mdl = parse_tfp(mdl_prob)\n",
        "    propagate_bound(parsed_mdl, [[0,1]])\n",
        "    xvars, yvars = embed_model(bkd, cplex, parsed_mdl, [[0,1]])\n",
        "\n",
        "    imp = yvars[0] - opt_mean - epsilon\n",
        "    stddev = pwl_exp(bkd, cplex, yvars[1], nnodes=11)\n",
        "    Z = cplex.continuous_var(lb=-cplex.infinity, ub=cplex.infinity, name=\"Z\")\n",
        "    cplex.add_constraint(Z * stddev == imp) # Z = imp / exp(yvars[1])\n",
        "    ncdf = pwl_normal_cdf(bkd, cplex, Z, nnodes=11)\n",
        "    npdf = pwl_normal_pdf(bkd, cplex, Z, nnodes=11)\n",
        "    ei = imp * ncdf + stddev * npdf\n",
        "    cplex.set_objective('min', ei)\n",
        "\n",
        "    cplex.set_time_limit(30)\n",
        "    sol = cplex.solve()\n",
        "    print(f'feasible: {sol is not None}')\n",
        "    if sol:\n",
        "        opt_x = sol.get_value(\"x0\")\n",
        "    print(sol)\n",
        "\n",
        "eml_test3()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "htzSCBgQ2ueh"
      },
      "outputs": [],
      "source": [
        "#@title Minimize UCB\n",
        "\n",
        "def eml_test4():\n",
        "    #train\n",
        "    p = Problem(None, polynomial_fun, [[0, 1]])\n",
        "    x,y = p.get_dataset(100)\n",
        "    x, y = x.ravel(), y.ravel()\n",
        "    x_train, y_train = x[:80], y[:80]\n",
        "    x_val, y_val = x[80:], y[80:]\n",
        "    mdl_prob = build_probabilistic_regressor(1)\n",
        "    mdl_prob.summary()\n",
        "    cb = [tf.keras.callbacks.EarlyStopping(patience=100, restore_best_weights=True)]\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "    bs = x_train.shape[0]\n",
        "    mdl_prob.compile(optimizer=optimizer, loss=dlambda_likelihood)\n",
        "    hstory = mdl_prob.fit(x_train, y_train, validation_data=(x_val, y_val), \n",
        "            batch_size=bs, epochs=500, verbose=0, callbacks=cb)\n",
        "    \n",
        "    #solve\n",
        "    beta = 0.01\n",
        "    cplex = cpx.Model()\n",
        "    bkd = cplex_backend.CplexBackend()\n",
        "\n",
        "    parsed_mdl = parse_tfp(mdl_prob)\n",
        "    propagate_bound(parsed_mdl, [[0,1]])\n",
        "    xvars, yvars = embed_model(bkd, cplex, parsed_mdl, [[0,1]])\n",
        "\n",
        "    ucb = cplex.continuous_var(lb=-cplex.infinity, ub=cplex.infinity, name=\"ucb\")\n",
        "    stddev = pwl_exp(bkd, cplex, yvars[1], nnodes=11)\n",
        "    cplex.add_constraint(ucb == -yvars[0] + beta * stddev) # max -f = min f\n",
        "\n",
        "    cplex.set_objective('max', ucb)\n",
        "\n",
        "    cplex.set_time_limit(30)\n",
        "    sol = cplex.solve()\n",
        "    print(f'feasible: {sol is not None}')\n",
        "    if sol:\n",
        "        opt_x = sol.get_value(\"x0\")\n",
        "        print(sol)\n",
        "    return sol, cplex\n",
        "eml_test4()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmRp8PdgLL4p"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TPC"
      ],
      "metadata": {
        "id": "tuVFrGT7TGiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ResNet18\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Dense, Conv2D,  MaxPool2D, Flatten, GlobalAveragePooling2D,  BatchNormalization, Layer, Add, Input, ReLU, Activation\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "\n",
        "def resnet_block(channels, down_sample=False):\n",
        "    strides = [2, 1] if down_sample else [1, 1]\n",
        "    KERNEL_SIZE = (3, 3)\n",
        "    INIT_SCHEME = \"he_normal\"\n",
        "\n",
        "    relu1 = ReLU()\n",
        "    relu2 = ReLU()\n",
        "\n",
        "    conv_1 = Conv2D(channels, strides=strides[0],\n",
        "                       kernel_size=KERNEL_SIZE, padding=\"same\", kernel_initializer=INIT_SCHEME)\n",
        "    bn_1 = BatchNormalization()\n",
        "    conv_2 = Conv2D(channels, strides=strides[1],\n",
        "                       kernel_size=KERNEL_SIZE, padding=\"same\", kernel_initializer=INIT_SCHEME)\n",
        "    bn_2 = BatchNormalization()\n",
        "    merge = Add()\n",
        "\n",
        "    if down_sample:\n",
        "        # perform down sampling using stride of 2, according to [1].\n",
        "        res_conv = Conv2D(\n",
        "            channels, strides=2, kernel_size=(1, 1), kernel_initializer=INIT_SCHEME, padding=\"same\")\n",
        "        res_bn = BatchNormalization()\n",
        "\n",
        "    def call(inputs):\n",
        "        res = inputs\n",
        "        x = conv_1(inputs)\n",
        "        x = bn_1(x)\n",
        "        x = Activation('relu')(x)\n",
        "        x = conv_2(x)\n",
        "        x = bn_2(x)\n",
        "\n",
        "        if down_sample:\n",
        "            res = res_conv(res)\n",
        "            res = res_bn(res)\n",
        "        # if not perform down sample, then add a shortcut directly\n",
        "        x = merge([x, res])\n",
        "        out = Activation('relu')(x)\n",
        "        return out\n",
        "\n",
        "    return call\n",
        "\n",
        "def build_resnet():\n",
        "    ks = 64\n",
        "    inputs = Input(shape=(32, 32, 3))\n",
        "    \n",
        "    relu1 = ReLU()\n",
        "    conv_1 = Conv2D(ks, (3, 3), strides=1,\n",
        "                       padding=\"same\", kernel_initializer=\"he_normal\")\n",
        "    init_bn = BatchNormalization()\n",
        "    pool_2 = MaxPool2D(pool_size=(2, 2), strides=2, padding=\"same\")\n",
        "    res_1_1 = resnet_block(ks)\n",
        "    res_1_2 = resnet_block(ks)\n",
        "    res_2_1 = resnet_block(ks*2, down_sample=True)\n",
        "    res_2_2 = resnet_block(ks*2)\n",
        "    res_3_1 = resnet_block(ks*4, down_sample=True)\n",
        "    res_3_2 = resnet_block(ks*4)\n",
        "    res_4_1 = resnet_block(ks*8, down_sample=True)\n",
        "    res_4_2 = resnet_block(ks*8)\n",
        "\n",
        "    avg_pool = GlobalAveragePooling2D()\n",
        "    flat = Flatten()\n",
        "    fc = Dense(10, activation=\"softmax\")\n",
        "\n",
        "    out = conv_1(inputs)\n",
        "    out = init_bn(out)\n",
        "    out = Activation('relu')(out)\n",
        "    out = pool_2(out)\n",
        "    for res_block in [res_1_1, res_1_2, res_2_1, res_2_2, res_3_1, res_3_2, res_4_1, res_4_2]:\n",
        "        out = res_block(out)\n",
        "    out = avg_pool(out)\n",
        "    out = flat(out)\n",
        "    out = fc(out)\n",
        "\n",
        "    return Model(inputs, out)"
      ],
      "metadata": {
        "id": "Hy-_qSWD1J3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CIFAR10 pretraining\n",
        "\n",
        "set_seed()\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "cifar10 = tf.keras.datasets.cifar10\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "\n",
        "dataset_mean = train_images.mean(axis=(0,1,2))\n",
        "dataset_std = train_images.std(axis=(0,1,2))\n",
        "\n",
        "# Normalize the input image so that each pixel value is between 0 to 1.\n",
        "train_images = (train_images - dataset_mean) / dataset_std\n",
        "test_images = (test_images - dataset_mean) / dataset_std\n",
        "\n",
        "# Build float model\n",
        "model = build_resnet()\n",
        "model.compile(optimizer=\"adam\",\n",
        "                loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "# Train float model\n",
        "model.fit(\n",
        "  train_images,\n",
        "  train_labels,\n",
        "  batch_size=128,\n",
        "  epochs=50,\n",
        "  callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", \n",
        "            patience=5, restore_best_weights=True)],\n",
        "  validation_split=0.1,\n",
        ")"
      ],
      "metadata": {
        "id": "WZKSvcSUTTgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Pretrained persistence: store\n",
        "\n",
        "model.save(\"pretrained_resnet18\")\n",
        "artifact = wandb.Artifact('resnet18', type='model')\n",
        "artifact.add_dir(\"pretrained_resnet18\")\n",
        "wandb.init(project='eml', name=\"artifact_resnet18\")\n",
        "wandb.log_artifact(artifact)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "M2WENPVaU3r6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Pretrained persistence: load\n",
        "\n",
        "! mkdir pretrained_resnet18\n",
        "! mkdir pretrained_resnet18/variables\n",
        "\n",
        "! wget https://api.wandb.ai/artifactsV2/gcp-us/veri/QXJ0aWZhY3Q6NTU2NTg0NjE=/d9d4d8f866df84014e528bb3c5617816 -O  pretrained_resnet18/variables/variables.data-00000-of-00001\n",
        "! wget https://api.wandb.ai/artifactsV2/gcp-us/veri/QXJ0aWZhY3Q6NTU2NTg0NjE=/4901af0e55327757ca7d7380b353279f -O  pretrained_resnet18/variables/variables.index\n",
        "! wget https://api.wandb.ai/artifactsV2/gcp-us/veri/QXJ0aWZhY3Q6NTU2NTg0NjE=/0a1a30ebb8498c7adaab17365283b563 -O  pretrained_resnet18/keras_metadata.pb\n",
        "! wget https://api.wandb.ai/artifactsV2/gcp-us/veri/QXJ0aWZhY3Q6NTU2NTg0NjE=/7c8a4682f521bac78f8a89b70342675b -O  pretrained_resnet18/saved_model.pb\n",
        "\n",
        "model = tf.keras.models.load_model(\"pretrained_resnet18\")"
      ],
      "metadata": {
        "id": "Q8dhRl-rWW7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Quantization definitions\n",
        "\n",
        "import tensorflow_model_optimization as tfmot\n",
        "from tensorflow_model_optimization.python.core.quantization.keras.default_8bit import default_8bit_quantize_scheme, default_8bit_quantize_registry, default_8bit_quantizers\n",
        "from tensorflow_model_optimization.quantization.keras.quantizers import LastValueQuantizer, MovingAverageQuantizer\n",
        "from tensorflow_model_optimization.quantization.keras import quantize_annotate_layer as ql\n",
        "from tensorflow_model_optimization.quantization.keras import quantize_annotate_model, QuantizeConfig\n",
        "\n",
        "\n",
        "class ConvWeightsQuantizer(LastValueQuantizer):\n",
        "  \"\"\"Quantizer for handling weights in Conv2D/DepthwiseConv2D layers.\"\"\"\n",
        "  def __init__(self, bits):\n",
        "        super(ConvWeightsQuantizer, self).__init__(\n",
        "            num_bits=bits, per_axis=True, symmetric=True, narrow_range=True)\n",
        "  def build(self, tensor_shape, name, layer):\n",
        "    min_weight = layer.add_weight(\n",
        "        name + '_min',\n",
        "        shape=(tensor_shape[-1],),\n",
        "        initializer=tf.keras.initializers.Constant(-6.0),\n",
        "        trainable=False)\n",
        "    max_weight = layer.add_weight(\n",
        "        name + '_max',\n",
        "        shape=(tensor_shape[-1],),\n",
        "        initializer=tf.keras.initializers.Constant(6.0),\n",
        "        trainable=False)\n",
        "    return {'min_var': min_weight, 'max_var': max_weight}\n",
        "\n",
        "class QConf(default_8bit_quantize_registry.Default8BitQuantizeConfig):\n",
        "    def __init__(self, bits, conv, *args, **kwargs):\n",
        "        super(QConf, self).__init__(*args, **kwargs)\n",
        "        self.bits = bits\n",
        "        if conv:\n",
        "            self.weight_quantizer = ConvWeightsQuantizer(bits)\n",
        "        else:\n",
        "            self.weight_quantizer = LastValueQuantizer(\n",
        "                num_bits=self.bits, per_axis=False, symmetric=True, narrow_range=True)\n",
        "        self.activation_quantizer = MovingAverageQuantizer(\n",
        "            num_bits=self.bits, per_axis=False, symmetric=False, narrow_range=False)\n",
        "\n",
        "class QAct(default_8bit_quantize_registry.Default8BitActivationQuantizeConfig):\n",
        "    def __init__(self, bits, *args, **kwargs):\n",
        "        super(QAct, self).__init__(*args, **kwargs)\n",
        "        self.bits = bits\n",
        "\n",
        "    def get_output_quantizers(self, layer):\n",
        "        self._assert_activation_layer(layer)\n",
        "        if not hasattr(layer.activation, '__name__'):\n",
        "            raise ValueError('Activation {} not supported by '\n",
        "                            'Default8BitActivationQuantizeConfig.'.format(\n",
        "                                layer.activation))\n",
        "        if layer.activation.__name__ in ['relu', 'swish']:\n",
        "            return [MovingAverageQuantizer(\n",
        "            num_bits=self.bits, per_axis=False, symmetric=False, narrow_range=False)]\n",
        "        elif layer.activation.__name__ in ['linear', 'softmax', 'sigmoid', 'tanh']:\n",
        "            return []\n",
        "        raise ValueError('Activation {} not supported by '\n",
        "                        'Default8BitActivationQuantizeConfig.'.format(\n",
        "                            layer.activation))\n",
        "\n",
        "class QReg(default_8bit_quantize_registry.Default8BitQuantizeRegistry):\n",
        "    def __init__(self, bitlist, *args, **kwargs):\n",
        "        super(QReg, self).__init__(*args, **kwargs)\n",
        "        self.bitlist = bitlist\n",
        "        self.counter = -1\n",
        "\n",
        "    def get_quantize_config(self, layer):\n",
        "        self.counter += 1\n",
        "        quantize_info = self._get_quantize_info(layer.__class__)\n",
        "        if layer.name.startswith('activation'):\n",
        "            return QAct(self.bitlist[self.counter])\n",
        "        elif layer.name.startswith('conv'):\n",
        "            return QConf(self.bitlist[self.counter], True,\n",
        "                     quantize_info.weight_attrs,\n",
        "                     quantize_info.activation_attrs,\n",
        "                     quantize_info.quantize_output)\n",
        "        return QConf(self.bitlist[self.counter], False,\n",
        "                     quantize_info.weight_attrs,\n",
        "                     quantize_info.activation_attrs,\n",
        "                     quantize_info.quantize_output)\n",
        "\n",
        "class QScheme(default_8bit_quantize_scheme.Default8BitQuantizeScheme):\n",
        "    def __init__(self, bitlist, *args, **kwargs):\n",
        "        super(QScheme, self).__init__(*args, **kwargs)\n",
        "        self.bitlist = bitlist\n",
        "\n",
        "    def get_quantize_registry(self):\n",
        "        return QReg(self.bitlist)"
      ],
      "metadata": {
        "id": "EITffA98XLGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title QAT\n",
        "\n",
        "bitlist = [3., 7., 2., 6., 2., 2., 5., 5., 6., 2., 2., 6., 2., 2., 5., 2., 4.,\n",
        "       2., 4., 7., 7., 2., 2., 2., 7., 3., 3., 2., 5., 2., 2., 5., 2., 8.,\n",
        "       5., 5., 2., 5., 6., 5., 7.]\n",
        "model = quantize_annotate_model(model)\n",
        "q_aware_model = tfmot.quantization.keras.quantize_apply(model, QScheme(bitlist))\n",
        "q_aware_model.compile(optimizer=\"adam\",\n",
        "                loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
        "q_aware_model.summary()\n",
        "\n",
        "q_aware_model.fit(train_images, train_labels,\n",
        "                  batch_size=512, epochs=5, validation_split=0.1,\n",
        "                  callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", \n",
        "                    patience=5, restore_best_weights=True)])\n",
        "\n",
        "model.compile(optimizer=\"adam\",\n",
        "                loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
        "# Compare float vs quantized accuracy\n",
        "_, baseline_model_accuracy = model.evaluate(\n",
        "    test_images, test_labels, verbose=0)\n",
        "\n",
        "q_aware_model_loss, q_aware_model_accuracy = q_aware_model.evaluate(\n",
        "   test_images, test_labels, verbose=0)\n",
        "\n",
        "print('Baseline test accuracy:', baseline_model_accuracy)\n",
        "print('Quant test accuracy:', q_aware_model_accuracy)\n",
        "\n",
        "# TFLite model\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "quantized_tflite_model = converter.convert()"
      ],
      "metadata": {
        "id": "cU6bv2_odDmC",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title TFLite vs TF model\n",
        "\n",
        "def evaluate_model(interpreter):\n",
        "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "  # Run predictions on every image in the \"test\" dataset.\n",
        "  prediction_digits = []\n",
        "  for i, test_image in enumerate(test_images):\n",
        "    if i % 1000 == 0:\n",
        "      print('Evaluated on {n} results so far.'.format(n=i))\n",
        "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "    # the model's input data format.\n",
        "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "    interpreter.set_tensor(input_index, test_image)\n",
        "\n",
        "    # Run inference.\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Post-processing: remove batch dimension and find the digit with highest\n",
        "    # probability.\n",
        "    output = interpreter.tensor(output_index)\n",
        "    digit = np.argmax(output()[0])\n",
        "    prediction_digits.append(digit)\n",
        "\n",
        "  print('\\n')\n",
        "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "  prediction_digits = np.array(prediction_digits)\n",
        "  accuracy = (prediction_digits == test_labels).mean()\n",
        "  return accuracy\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "test_accuracy = evaluate_model(interpreter)\n",
        "\n",
        "print('Quant TFLite test_accuracy:', test_accuracy)\n",
        "print('Quant TF test accuracy:', q_aware_model_accuracy)"
      ],
      "metadata": {
        "id": "YNXPhJyGd2TF",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Compare size\n",
        "\n",
        "import tempfile\n",
        "\n",
        "# # Create float TFLite model.\n",
        "float_converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "float_tflite_model = float_converter.convert()\n",
        "\n",
        "# # Measure sizes of models.\n",
        "_, float_file = tempfile.mkstemp('.tflite')\n",
        "_, quant_file = tempfile.mkstemp('.tflite')\n",
        "\n",
        "with open(quant_file, 'wb') as f:\n",
        "  f.write(quantized_tflite_model)\n",
        "\n",
        "with open(float_file, 'wb') as f:\n",
        "  f.write(float_tflite_model)\n",
        "\n",
        "print(\"Float model in Mb:\", os.path.getsize(float_file) / float(2**20))\n",
        "print(\"Quantized model in Mb:\", os.path.getsize(quant_file) / float(2**20))"
      ],
      "metadata": {
        "id": "tUcmnA8LeriD",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "nNNmQ5x5YJaS",
        "JcyNrxy43Bgb",
        "txjrAWgNO1RE",
        "C00w0zpdczA4",
        "cUbgkjkyrII_",
        "l1WCq0t24LeC"
      ],
      "name": "experiments.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}